{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_dataframe(doc_path = '../data/Rus_Ukr_war_data.json'):\n",
    "    with open(doc_path) as fp:\n",
    "        lines = fp.readlines()\n",
    "    df=pd.read_json(doc_path, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_dataset(raw_df):\n",
    "    # Select only relevant columns\n",
    "    clean_df = raw_df[[\"created_at\",\"id\",\"full_text\",\"entities\",\"favorite_count\",\"retweet_count\",\"user\"]]\n",
    "\n",
    "    # Rename columns\n",
    "    renames = {\"created_at\":\"date\", \"full_text\":\"tweet\", \"favorite_count\":\"likes\",\"retweet_count\":\"retweets\", \"id\":\"tweet_id\"}\n",
    "    clean_df = clean_df.rename(columns=renames)\n",
    "\n",
    "    # Create Series of list of hashtags from `entities` object\n",
    "    df_hashtags = pd.json_normalize(clean_df[\"entities\"])[\"hashtags\"]\n",
    "    df_hashtags = df_hashtags.apply(lambda x: [item[\"text\"] for item in x])\n",
    "\n",
    "    # Create Series of username ids\n",
    "    df_user = pd.json_normalize(clean_df[\"user\"])[\"id\"].rename(\"user_id\")\n",
    "\n",
    "    # Merge hashtags and username columns to the DataFrame\n",
    "    clean_df = pd.concat([clean_df,df_hashtags,df_user], axis=1).drop(columns=[\"entities\",\"user\"])\n",
    "\n",
    "    # Create URL column manually from the user id and tweet id columns\n",
    "    clean_df[\"url\"] = \"https://twitter.com/\" + clean_df[\"user_id\"].astype(str) + \"/status/\" + clean_df[\"tweet_id\"].astype(str)\n",
    "\n",
    "    # Extract tags to other users from the tweet body\n",
    "    clean_df[\"tags\"] = clean_df[\"tweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "    # Returns a DataFrame of tweets with columns [\"date\", \"tweet_id\", \"tweet\", \"likes\", \"retweets\", \"hashtags\", \"user_id\", \"url\", \"tags\", \"tags\"]\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictograms\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "\n",
    "def clean_tweet(line):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = re.sub(r'[.,;:!?\"\\'-@]', '', line).replace(\"#\", \"\").replace(\"’\", \"\").replace(\"“\", \"\").replace(\"\\n\",\" \")\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = remove_emojis(line).strip().replace(\"  \", \" \")\n",
    "    line = line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    line =[word for word in line if word not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line =[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = [word for word in line if word != \"\"]\n",
    "    ## END CODE\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def process_text_column(column):\n",
    "    column = column.apply(clean_tweet)\n",
    "    return column\n",
    "\n",
    "def join_docs_tweets_dfs(tweets, csv_file='../data/Rus_Ukr_war_data_ids.csv'):\n",
    "    docs = pd.read_csv(csv_file, sep=\"\\t\", header=None)\n",
    "    docs = docs.rename(columns={0:\"doc_id\",1:\"tweet_id\"})\n",
    "    tweets = tweets.join(docs.set_index('tweet_id'), on='tweet_id')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "\n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                term_docs = [posting[0] for posting in self.index[term]]\n",
    "                term_docs = set(term_docs)\n",
    "                if len(docs)==0:\n",
    "                    docs = docs.union(term_docs)\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                pass\n",
    "        docs = list(docs)\n",
    "        return docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TfIdfIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text, num_documents):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = defaultdict(float)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            for term, posting in current_page_index.items():\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "                self.df[term] += 1\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query))\n",
    "        query_vector = [0] * len(stemmed_query)\n",
    "\n",
    "        # compute the norm for the query tf\n",
    "        query_terms_count = collections.Counter(stemmed_query)  # get the frequency of each term in the query.\n",
    "        # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "\n",
    "        query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "        for termIndex, term in enumerate(stemmed_query):  #termIndex is the index of the term in the query\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "\n",
    "            # TODO: check how to vectorize the query\n",
    "            # query_vector[termIndex]=idf[term]  # original\n",
    "            ## Compute tf*idf(normalize TF as done with documents)\n",
    "            query_vector[termIndex] = query_terms_count[term] / query_norm * self.idf[term]\n",
    "\n",
    "            # Generate doc_vectors for matching docs\n",
    "            for doc_index, (doc, postings) in enumerate(self.index[term]):\n",
    "                # Example of [doc_index, (doc, postings)]\n",
    "                # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "                # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "                # term is in doc 26 in positions 1,4, .....\n",
    "                # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "                #tf[term][0] will contain the tf of the term \"term\" in the doc 26\n",
    "                if doc in unranked_results:\n",
    "                    doc_vectors[doc][termIndex] = self.tf[term][doc_index] * self.idf[term] \n",
    "\n",
    "        # Calculate the score of each doc\n",
    "        # compute the cosine similarity between queyVector and each docVector:\n",
    "        # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "        # see np.dot\n",
    "\n",
    "        doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        #print(doc_scores)\n",
    "        result_docs = [x[1] for x in doc_scores]\n",
    "        #print document titles instead if document id's\n",
    "        #result_docs=[ title_index[x] for x in result_docs ]\n",
    "\n",
    "        return result_docs\n",
    "                                          \n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                if len(docs)==0:\n",
    "                    docs = docs.union(term_docs)\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        ranked_docs = self.rank(query, docs)\n",
    "\n",
    "        return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets in the corpus: 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30 18:39:17+00:00</td>\n",
       "      <td>1575918221013979136</td>\n",
       "      <td>@MelSimmonsFCDO Wrong. Dictator Putin's Fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RussiainvadesUkraine, UkraineRussiaWar]</td>\n",
       "      <td>1404526426330701825</td>\n",
       "      <td>https://twitter.com/1404526426330701825/status...</td>\n",
       "      <td>[MelSimmonsFCDO]</td>\n",
       "      <td>[melsimmonsfcdo, wrong, dictat, putin, fascist...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-30 18:38:44+00:00</td>\n",
       "      <td>1575918081461080065</td>\n",
       "      <td>🇺🇦❤️ The Armed Forces liberated the village of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...</td>\n",
       "      <td>1257116113898536961</td>\n",
       "      <td>https://twitter.com/1257116113898536961/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[arm, forc, liber, villag, drobyshev, lymansk,...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30 18:38:23+00:00</td>\n",
       "      <td>1575917992390823936</td>\n",
       "      <td>ALERT 🚨Poland preps anti-radiation tablets ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[NATO, Putin, Russia, RussiaInvadedUkraine, Uk...</td>\n",
       "      <td>1460003892415053828</td>\n",
       "      <td>https://twitter.com/1460003892415053828/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alert, poland, prep, antiradi, tablet, nuclea...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-30 18:38:03+00:00</td>\n",
       "      <td>1575917907774967809</td>\n",
       "      <td>I’m still waiting for my google map 🗺️ to upda...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Putin, UkraineRussiaWar]</td>\n",
       "      <td>285766081</td>\n",
       "      <td>https://twitter.com/285766081/status/157591790...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[im, still, wait, googl, map, updat, russia, n...</td>\n",
       "      <td>doc_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-30 18:37:56+00:00</td>\n",
       "      <td>1575917878410301441</td>\n",
       "      <td>@EmmanuelMacron probably you're right or you h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[European, UkraineRussiaWar]</td>\n",
       "      <td>1537193346107686915</td>\n",
       "      <td>https://twitter.com/1537193346107686915/status...</td>\n",
       "      <td>[EmmanuelMacron]</td>\n",
       "      <td>[emmanuelmacron, probabl, your, right, say, an...</td>\n",
       "      <td>doc_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date             tweet_id  \\\n",
       "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
       "1 2022-09-30 18:38:44+00:00  1575918081461080065   \n",
       "2 2022-09-30 18:38:23+00:00  1575917992390823936   \n",
       "3 2022-09-30 18:38:03+00:00  1575917907774967809   \n",
       "4 2022-09-30 18:37:56+00:00  1575917878410301441   \n",
       "\n",
       "                                               tweet  likes  retweets  \\\n",
       "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
       "1  🇺🇦❤️ The Armed Forces liberated the village of...      0         0   \n",
       "2  ALERT 🚨Poland preps anti-radiation tablets ove...      0         0   \n",
       "3  I’m still waiting for my google map 🗺️ to upda...      0         0   \n",
       "4  @EmmanuelMacron probably you're right or you h...      0         0   \n",
       "\n",
       "                                            hashtags              user_id  \\\n",
       "0           [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
       "1  [Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...  1257116113898536961   \n",
       "2  [NATO, Putin, Russia, RussiaInvadedUkraine, Uk...  1460003892415053828   \n",
       "3                          [Putin, UkraineRussiaWar]            285766081   \n",
       "4                       [European, UkraineRussiaWar]  1537193346107686915   \n",
       "\n",
       "                                                 url              tags  \\\n",
       "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
       "1  https://twitter.com/1257116113898536961/status...                []   \n",
       "2  https://twitter.com/1460003892415053828/status...                []   \n",
       "3  https://twitter.com/285766081/status/157591790...                []   \n",
       "4  https://twitter.com/1537193346107686915/status...  [EmmanuelMacron]   \n",
       "\n",
       "                                       stemmed_tweet doc_id  \n",
       "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n",
       "1  [arm, forc, liber, villag, drobyshev, lymansk,...  doc_2  \n",
       "2  [alert, poland, prep, antiradi, tablet, nuclea...  doc_3  \n",
       "3  [im, still, wait, googl, map, updat, russia, n...  doc_4  \n",
       "4  [emmanuelmacron, probabl, your, right, say, an...  doc_5  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_path = '../data/Rus_Ukr_war_data.json'\n",
    "csv_path = '../data/Rus_Ukr_war_data_ids.csv'\n",
    "\n",
    "# Import from JSON file\n",
    "raw_df = from_json_to_dataframe(doc_path)\n",
    "\n",
    "# Clean raw DataFrame to have a more convenient structure\n",
    "clean_df = clean_raw_dataset(raw_df)\n",
    "\n",
    "# \n",
    "clean_df[\"stemmed_tweet\"] = process_text_column(clean_df[\"tweet\"])\n",
    "\n",
    "\n",
    "clean_df = join_docs_tweets_dfs(clean_df, csv_path)\n",
    "\n",
    "\n",
    "print(\"Total number of Tweets in the corpus: {}\".format(len(clean_df)))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = InvertedIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "\n",
      "Query: \n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 0 for the searched query:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "print(\"Query:\", query)\n",
    "results = inverted_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(results)))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id = {}\\ntweet = {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute after implementing TFIDF INDEX\n",
    "\n",
    "num_tweets = len(clean_df)\n",
    "tf_idf_index = TfIdfIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"], num_documents=num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "russia tank\n",
      "Query: russia tank\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 44 for the searched query:\n",
      "\n",
      "tweet_id= 1575243962843107328\n",
      "tweet: This is the first tank on tank footage I have come across in #Ukraine #UkraineRussiaWar #Russia #UkraineRussia https://t.co/76q7egBM6J\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575388141653196800\n",
      "tweet: 🇷🇺A tank of Pacific Fleet marines vs a Ukrainian tank. \n",
      "\n",
      "The Ukrainian tank was destroyed by a direct hit.\n",
      "\n",
      "#UkrainianArmy  #UkraineRussiaWar #UkraineWar #Ukraine️ #RussianUkrainianWar #RussianArmy #Russia #RussiaUkraineWar https://t.co/dtdqT7AWBm\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575577950316941313\n",
      "tweet: Russian tank was destroyed by Ukrainian tank fire. \n",
      "\n",
      "#ukraine #russia #ukrainewar #ukrainenews #ukrainerussiawar #russianwar #ukrainearmy #russiaarmy #todaynews #news https://t.co/E0Y9CI3Mvf\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575540725227597824\n",
      "tweet: Footage Russian tank destroyed by Ukrainian tank 👏\n",
      "#Russian #Russia #Ukraine #Ukrainian #UkraineWar #UkraineRussiaWar #RussiaIsATerroristState https://t.co/dXXl0T7W6K\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575843161897062400\n",
      "tweet: The 🇺🇦95th Brigade has demilitarized another 🇷🇺tank today.\n",
      "#Ukraine #UkraineRussiaWar #Ukrainians #UkrainianArmy #Russia #Russian #Russians #RussianArmy #RussianMobilization #demobilization #tanks https://t.co/x8OABta53Y\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575324698770083842\n",
      "tweet: Why does this happen to Russian tanks\n",
      "\n",
      "#russia #UkraineRussiaWar \n",
      "https://t.co/KVdG5Q7XQE\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575547576560062470\n",
      "tweet: Another 🇷🇺tank is demilitarized in eastern Ukraine.💪🇺🇦\n",
      "#Ukraine #Ukrainians #UkrainianArmy #RussiaUkraineWar #UkraineRussiaWar #RussiaInvadedUkraine #Russia #Russians #RussianArmy #MobilizationInRussia #mobilization #demobilization #Tanks https://t.co/KYNozFbvMP\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575274301175263232\n",
      "tweet: Evacuation of the captured tank 2 Soldiers of the 1st Separate Tank Brigade of the Siver despite the danger of shelling managed to take the Russian armored from the gray zone\n",
      "\n",
      "#Russia #UkraineRussianWar #UkraineRussiaWar #UkraineWar #UkrainianArmy #Ukraine https://t.co/fMx1yjOTTC\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575614292358938625\n",
      "tweet: Ukraine where Russian tanks go to die, or be captured though not in this case.\n",
      "Again lone tank with no support, abysmal tactics.\n",
      "#Ukraine #UkraineWar #UkrainianArmy #UkraineRussiaWar #UkraineWillWin #Russia #Russian #RussianArmy https://t.co/iCbrw16vsp\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id= 1575156677149597701\n",
      "tweet: Russian tank T-90 can not swim!\n",
      "\n",
      "#UkraineRussiaWar #Ukraine #Russia #RussiaUkraineWar https://t.co/S4kkn969we\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Also execute after having implemented TFIDF INDEX\n",
    "\n",
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "print(\"Query:\", query)\n",
    "\n",
    "results = tf_idf_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(results)))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION PART\n",
    "\n",
    "# We need to import the evaluation_gt file\n",
    "# I think it is a just a csv, so just pandas read csv\n",
    "\n",
    "# We have to do two separate evaluations\n",
    "\n",
    "# First, running the queries in the pdf (they call them information needs)\n",
    "# and then computing the P@K, R@K, etc. for the 3 queries they propose\n",
    "\n",
    "# Second, inventing two new queries, and assessing ourselves if the top N results given by \n",
    "# our algorithm are relevant (1) or not (0), and then computing P@K, R@K, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1452</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2908</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_618</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_489</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_110</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc query_id  label\n",
       "0  doc_1452       Q3      1\n",
       "1  doc_2908       Q3      1\n",
       "2   doc_618       Q3      1\n",
       "3   doc_489       Q3      1\n",
       "4   doc_110       Q3      1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df = pd.read_csv(\"../data/Evaluation_gt.csv\")\n",
    "\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY1 = \"tank Kharkiv\" # What is the discussion regarding a tank in Kharkiv?\n",
    "QUERY2 = \"nord stream\" # What discussion are there about the Nord Stream pipeline?\n",
    "QUERY3 = \"territories annexation russia\" # What is being said about the annexation of territories in Russia?\n",
    "\n",
    "QUERY4 = \"\" # \n",
    "QUER5 = \"putin kill\" # Are there discussions or messages about killing president Putin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k = 10): # P@K\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    relevant = sum(doc_score == 1)\n",
    "\n",
    "    return relevant / k\n",
    "\n",
    "\n",
    "def recall_at_k(doc_score, y_score, k = 10): # R@K\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score_k = np.take(doc_score, order[:k])\n",
    "    relevant = sum(doc_score_k == 1)\n",
    "    \n",
    "    doc_score_total = np.take(doc_score, order)\n",
    "    relevant_total = sum(doc_score == 1)\n",
    "    return float(relevant) / relevant_total if relevant_total>0 else 0\n",
    "\n",
    "def avg_precision_at_k(doc_score, y_score, k=10): #Average Precision@K\n",
    "\n",
    "    doc_score = np.array(doc_score)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    gtp = np.sum(doc_score == 1)\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1:\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i + 1)\n",
    "    return prec_at_i / gtp\n",
    "\n",
    "def f1_score(precision, recall): # F1-Score@K\n",
    "\n",
    "    if precision + recall == 0:  # Avoid division by zero\n",
    "        return 0 \n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def mean_avg_precision(avg_precisions): #Mean Average Precision (MAP)\n",
    "\n",
    "    return sum(avg_precisions) / len(avg_precisions)\n",
    "\n",
    "def rr_at_k(doc_score, y_score, k=10):\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\"\n",
    "\n",
    "def mean_reciprocal_rank(rr_ranks): # MRR\n",
    "    \n",
    "    return sum(rr_ranks) / len(rr_ranks)\n",
    "\n",
    "\n",
    "def discounted_cumulative_gain(doc_score, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** doc_score - 1  # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator\n",
    "    return np.sum(gain / discounts)  #return dcg@k\n",
    "\n",
    "\n",
    "def normalized_discounted_cumulative_gain(doc_score, y_score, k=10):\n",
    "    dcg_max = discounted_cumulative_gain(doc_score, doc_score, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(discounted_cumulative_gain(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMING QUERY: tank Kharkiv\n",
      "--------------------Currently analysing: Q1 \n",
      "\n",
      "\n",
      "Using standard Inverted Index...\n",
      "\n",
      "P@K for Q1 query: 0.2\n",
      "R@K for Q1 query: 0.2\n",
      "AP@K for Q1 query: 0.0496031746031746\n",
      "F1-Score@K for Q1 query: 0.23529411764705882\n",
      "NDCG for Q1 query: 0\n",
      "\n",
      "\n",
      "\n",
      "PERFORMING QUERY: nord stream\n",
      "--------------------Currently analysing: Q2 \n",
      "\n",
      "\n",
      "Using standard Inverted Index...\n",
      "\n",
      "P@K for Q2 query: 0.3\n",
      "R@K for Q2 query: 0.3\n",
      "AP@K for Q2 query: 0.15\n",
      "F1-Score@K for Q2 query: 0.3\n",
      "NDCG for Q2 query: 0\n",
      "\n",
      "\n",
      "\n",
      "PERFORMING QUERY: territories russia\n",
      "--------------------Currently analysing: Q3 \n",
      "\n",
      "\n",
      "Using standard Inverted Index...\n",
      "\n",
      "P@K for Q3 query: 0.0\n",
      "R@K for Q3 query: 0.0\n",
      "AP@K for Q3 query: 0\n",
      "F1-Score@K for Q3 query: 0\n",
      "NDCG for Q3 query: 0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "MAP: 0.16666666666666666\n",
      "MRR: 0.20833333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-102-21ccef4d1967>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth_aux[\"label\"] = 0\n",
      "<ipython-input-102-21ccef4d1967>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth_aux[\"label\"] = 0\n",
      "<ipython-input-102-21ccef4d1967>:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth_aux[\"label\"] = 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "queries = {\"Q1\": QUERY1,\"Q2\":QUERY2,\"Q3\":QUERY3}\n",
    "k = 10\n",
    "avg_precisions = []\n",
    "rr_ranks = []\n",
    "\n",
    "for current_query in queries.keys():\n",
    "    print(\"PERFORMING QUERY:\", queries[current_query])\n",
    "    print(f\"--------------------Currently analysing: {current_query} \\n\")\n",
    "#warnings.filterwarnings(\"ignore\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = evaluation_df.loc[evaluation_df[\"query_id\"]==current_query]\n",
    "    ground_truth_aux = evaluation_df.loc[(evaluation_df[\"query_id\"]!=current_query)&(evaluation_df[\"label\"]==1)]\n",
    "    ground_truth_aux[\"label\"] = 0\n",
    "    ground_truth = pd.concat([ground_truth, ground_truth_aux])\n",
    "    ground_truth = ground_truth.merge(clean_df, left_on='doc', right_on='doc_id', how='left')\n",
    "#warnings.filterwarnings(\"default\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = ground_truth[[\"tweet_id\",\"label\"]].rename(columns={\"label\": \"true_score\"})\n",
    "    \n",
    "\n",
    "\n",
    "    print(\"\\nUsing standard Inverted Index...\\n\")\n",
    "    results = inverted_index.search(queries[current_query])\n",
    "    results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":reversed(range(1, len(results)+1))})\n",
    "    results = results.merge(ground_truth, on='tweet_id', how='left')\n",
    "\n",
    "    # In results we have tweet_ids, predicted_scores and true_score\n",
    "    precision_atk = precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    avg_precisions.append(precision_atk)\n",
    "    print(f\"P@K for {current_query} query: {precision_atk}\")\n",
    "\n",
    "    recall_atk = recall_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"R@K for {current_query} query: {precision_atk}\")\n",
    "\n",
    "    avg_precision_atk = avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"AP@K for {current_query} query: {avg_precision_atk}\")\n",
    "\n",
    "    f1score = f1_score(precision_atk, recall_atk)\n",
    "    print(f\"F1-Score@K for {current_query} query: {f1score}\")\n",
    "\n",
    "    rr_atk = rr_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    rr_ranks.append(rr_atk)\n",
    "    \n",
    "    ndcg_at_k = normalized_discounted_cumulative_gain(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"NDCG for {current_query} query: {ndcg_at_k}\")\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "mean_avg_precision_at_k = mean_avg_precision(avg_precisions)\n",
    "print(\"MAP:\", mean_avg_precision_at_k)\n",
    "\n",
    "mean_rr_ranks = mean_reciprocal_rank(rr_ranks)\n",
    "print(\"MRR:\", mean_rr_ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1575913861957369856,\n",
       " 1575917907774967809,\n",
       " 1575913773818605568,\n",
       " 1575910831191035904,\n",
       " 1575912858004561920,\n",
       " 1575917748978434053,\n",
       " 1575828107721834498,\n",
       " 1575817942356197377,\n",
       " 1575463145610559491,\n",
       " 1575340007551811590,\n",
       " 1575574291591987200,\n",
       " 1575226859645960192,\n",
       " 1575861425133981696,\n",
       " 1575836603863699458,\n",
       " 1575241438937239554,\n",
       " 1575154617620504576,\n",
       " 1575837833268715522,\n",
       " 1575167560835661827,\n",
       " 1575633135190290432,\n",
       " 1575579616689139712,\n",
       " 1575177752931868672,\n",
       " 1575914207052283925,\n",
       " 1575496332701114372,\n",
       " 1575431091766411265,\n",
       " 1575170200852647936,\n",
       " 1575892959572418560,\n",
       " 1575861230102798336,\n",
       " 1575825066994327552,\n",
       " 1575816865401954304,\n",
       " 1575836501539454976,\n",
       " 1575812330465951744,\n",
       " 1575647395157016576,\n",
       " 1575185571131326464,\n",
       " 1575685368619225089,\n",
       " 1575852450577551361,\n",
       " 1575850518714294273,\n",
       " 1575644512033964032,\n",
       " 1575844656583086080,\n",
       " 1575829074429157376,\n",
       " 1575697533745364992,\n",
       " 1575664913938513920,\n",
       " 1575880067099480064,\n",
       " 1575898746868695040,\n",
       " 1575814426787450880,\n",
       " 1575504452299083777,\n",
       " 1575447524000911360]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverted_index.search(QUERY3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "--------------------Currently analysing: Q1 \n",
      "\n",
      "              tweet_id  true_score\n",
      "0  1575528927245770752           1\n",
      "\n",
      "Using standard Inverted Index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-73-07a43a26c179>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth_aux[\"label\"] = 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-07a43a26c179>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;31m# In results we have tweet_ids, predicted_scores and true_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0mprecision_atk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprecision_at_k\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"true_score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"predicted_score\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mavg_precisions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprecision_atk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"P@K for {current_query} query: {precision_at_k}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": [
    "# print(\"\\nUsing TF-IDF Index...\")\n",
    "# results = tf_idf_index.search(QUERY1)\n",
    "# results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":range(1, len(results)+1)})\n",
    "# results = results.merge(ground_truth, on='tweet_id', how='left')\n",
    "# print(results.head(5), \"\\n...\")\n",
    "# # In results we have tweet_ids, predicted_scores and true_score\n",
    "# # TODO: CALL ALL EVALUATION FUNCTIONS AND PRINT THEIR RESULTS\n",
    "# print(\"P@K:\", precision_at_k(results[\"true_score\"], results[\"predicted_score\"]))\n",
    "# print(\"AP@K:\", avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"]))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "queries = [\"Q1\",\"Q2\",\"Q3\"]\n",
    "k = 10\n",
    "avg_precisions = []\n",
    "rr_ranks = []\n",
    "\n",
    "for current_query in queries:\n",
    "\n",
    "    print(f\"--------------------Currently analysing: {current_query} \\n\")\n",
    "#warnings.filterwarnings(\"ignore\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = evaluation_df.loc[evaluation_df[\"query_id\"]==current_query]\n",
    "    ground_truth_aux = evaluation_df.loc[(evaluation_df[\"query_id\"]!=current_query)&(evaluation_df[\"label\"]==1)]\n",
    "    ground_truth_aux[\"label\"] = 0\n",
    "    ground_truth = pd.concat([ground_truth, ground_truth_aux])\n",
    "    ground_truth = ground_truth.merge(clean_df, left_on='doc', right_on='doc_id', how='left')\n",
    "#warnings.filterwarnings(\"default\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = ground_truth[[\"tweet_id\",\"label\"]].rename(columns={\"label\": \"true_score\"})\n",
    "    print(ground_truth.head(1))\n",
    "\n",
    "\n",
    "    print(\"\\nUsing standard Inverted Index...\")\n",
    "    results = tf_idf_index.search(QUERY1)\n",
    "    results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":reversed(range(1, len(results)+1))})\n",
    "    results = results.merge(ground_truth, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "    # In results we have tweet_ids, predicted_scores and true_score\n",
    "\n",
    "    precision_atk = precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    avg_precisions.append(precision_atk)\n",
    "    print(f\"P@K for {current_query} query: {precision_atk}\")\n",
    "\n",
    "    recall_at_k = recall_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"R@K for {current_query} query: {precision_at_k}\")\n",
    "\n",
    "    avg_precision_at_k = avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"AP@K for {current_query} query: {avg_precision_at_k}\")\n",
    "\n",
    "    f1_score = f1_score(precision_at_k, recall_at_k)\n",
    "    print(f\"F1-Score@K for {current_query} query: {f1_score}\")\n",
    "\n",
    "    rr_at_k = rr_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    rr_ranks.append(rr_at_k)\n",
    "    \n",
    "    ndcg_at_k = normalized_discounted_cumulative_gain(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"NDCG for {current_query} query: {ndcg_at_k}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "mean_avg_precision_at_k = mean_avg_precision(avg_precisions)\n",
    "print(\"MAP:\", mean_avg_precision_at_k)\n",
    "\n",
    "mean_rr_ranks = mean_reciprocal_rank(rr_ranks)\n",
    "print(\"MRR:\", mean_rr_ranks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
