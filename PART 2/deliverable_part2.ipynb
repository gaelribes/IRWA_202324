{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_dataframe(doc_path = '../data/Rus_Ukr_war_data.json'):\n",
    "    with open(doc_path) as fp:\n",
    "        lines = fp.readlines()\n",
    "    df=pd.read_json(doc_path, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_dataset(raw_df):\n",
    "    # Select only relevant columns\n",
    "    clean_df = raw_df[[\"created_at\",\"id_str\",\"full_text\",\"entities\",\"favorite_count\",\"retweet_count\",\"user\"]]\n",
    "\n",
    "    # Rename columns\n",
    "    renames = {\"created_at\":\"date\", \"full_text\":\"tweet\", \"favorite_count\":\"likes\",\"retweet_count\":\"retweets\", \"id_str\":\"tweet_id\"}\n",
    "    clean_df = clean_df.rename(columns=renames)\n",
    "\n",
    "    # Create Series of list of hashtags from `entities` object\n",
    "    df_hashtags = pd.json_normalize(clean_df[\"entities\"])[\"hashtags\"]\n",
    "    df_hashtags = df_hashtags.apply(lambda x: [item[\"text\"] for item in x])\n",
    "\n",
    "    # Create Series of username ids\n",
    "    df_user = pd.json_normalize(clean_df[\"user\"])[\"id\"].rename(\"user_id\")\n",
    "\n",
    "    # Merge hashtags and username columns to the DataFrame\n",
    "    clean_df = pd.concat([clean_df,df_hashtags,df_user], axis=1).drop(columns=[\"entities\",\"user\"])\n",
    "\n",
    "    # Create URL column manually from the user id and tweet id columns\n",
    "    clean_df[\"url\"] = \"https://twitter.com/\" + clean_df[\"user_id\"].astype(str) + \"/status/\" + clean_df[\"tweet_id\"].astype(str)\n",
    "\n",
    "    # Extract tags to other users from the tweet body\n",
    "    clean_df[\"tags\"] = clean_df[\"tweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "    # Returns a DataFrame of tweets with columns [\"date\", \"tweet_id\", \"tweet\", \"likes\", \"retweets\", \"hashtags\", \"user_id\", \"url\", \"tags\", \"tags\"]\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictograms\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "\n",
    "def clean_tweet(line):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = re.sub(r'[.,;:!?\"\\'-@]', '', line).replace(\"#\", \"\").replace(\"’\", \"\").replace(\"“\", \"\").replace(\"\\n\",\" \")\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = remove_emojis(line).strip().replace(\"  \", \" \")\n",
    "    line = line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    line =[word for word in line if word not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line =[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = [word for word in line if word != \"\"]\n",
    "    ## END CODE\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def process_text_column(column):\n",
    "    column = column.apply(clean_tweet)\n",
    "    return column\n",
    "\n",
    "def join_docs_tweets_dfs(tweets, csv_file='../data/Rus_Ukr_war_data_ids.csv'):\n",
    "    docs = pd.read_csv(csv_file, sep=\"\\t\", header=None)\n",
    "    docs = docs.rename(columns={0:\"doc_id\",1:\"tweet_id\"})\n",
    "    tweets = tweets.join(docs.set_index('tweet_id'), on='tweet_id')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 3}\n"
     ]
    }
   ],
   "source": [
    "term_docs = {1, 2, 3, 4, 6}\n",
    "b = {1, 3}\n",
    "\n",
    "c = term_docs.intersection(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):  # Remember, lines contain all documents from file\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            ## ===============================================================\n",
    "            ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "            ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "            ## Example: if the curr_doc has id 1 and its text is \"web retrieval information retrieval\":\n",
    "\n",
    "            ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "            ## the term ‘web’ appears in document 1 in positions 0,\n",
    "            ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "            ## ===============================================================\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "                try:\n",
    "                    # if the term is already in the index for the current page (current_page_index)\n",
    "                    # append the position to the corresponding list (elemento 1 del arreglo, el 0 es la id del documento)\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "            # merge the current page index with the main index\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "    def search(self, query):\n",
    "        query = clean_tweet(query) #so that stemed terms are matched in the index\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = [posting[0] for posting in self.index[term]]\n",
    "                # docs = docs Union term_docs\n",
    "                # docs |= set(term_docs)\n",
    "                # MARC: Documents information: Since we are dealing with conjunctive queries (AND),\n",
    "                # each of the returned documents should contain all the words in the query. -> The intersection\n",
    "                term_docs = set(term_docs)\n",
    "                docs = term_docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        docs = list(docs)\n",
    "        return docs\n",
    "\n",
    "class TfIdfIndex():\n",
    "\n",
    "    def __init__(self, lines, num_documents):\n",
    "\n",
    "        # TODO: Copy the create_index_tfidf function from lab 1\n",
    "        # You can also use the class InvertedIndex above as reference as to how to code classes in Python\n",
    "        # Have in mind that __init__ should not return anything. The variables that...\n",
    "        # create_index_tfidf returns (index, tf, df, idf) should be stored in the class using:\n",
    "        # self.index, self.tf, etc. We don't need to store title_index, it was just for the lab1\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)  #term frequencies of terms in documents (documents in the same order as in the main index)\n",
    "        self.df = defaultdict(int)  #document frequencies of terms in the corpus\n",
    "        self.idf = defaultdict(float)\n",
    "\n",
    "        for line in lines:\n",
    "            line_arr = line.split(\"|\")\n",
    "            page_id = int(line_arr[0])\n",
    "            terms = build_terms(''.join(line_arr[1:]))  #page_title + page_text\n",
    "\n",
    "            ## ===============================================================\n",
    "            ## create the index for the **current page** and store it in current_page_index\n",
    "            ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "            ## Example: if the curr_doc has id 1 and its text is\n",
    "            ##\"web retrieval information retrieval\":\n",
    "\n",
    "            ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "            ## the term ‘web’ appears in document 1 in positions 0,\n",
    "            ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "            ## ===============================================================\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):  ## terms contains page_title + page_text\n",
    "                try:\n",
    "                    # if the term is already in the dict append the position to the corresponding list\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "            # normalize term frequencies\n",
    "            # Compute the denominator to normalize term frequencies (formula 2 above)\n",
    "            # norm is the same for all terms of a document.\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                # posting will contain the list of positions for current term in current document.\n",
    "                # posting ==> [current_doc, [list of positions]]\n",
    "                # you can use it to infer the frequency of current term.\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "            for term, posting in current_page_index.items():\n",
    "                # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4)) ## SEE formula (1) above\n",
    "                #increment the document frequency of current term (number of documents containing the current term)\n",
    "                self.df[term] += 1 # increment DF for current term\n",
    "\n",
    "            #merge the current page index with the main index\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            # Compute IDF following the formula (3) above. HINT: use np.log\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "\n",
    "        # TODO: Copy the rank_documents function from lab 1\n",
    "        # Have in mind that we don't need to pass index, idf nor tf as they are stored in the class using\n",
    "        # self.index, self.idf, etc.\n",
    "        # Note that terms is called here stemmed query and docs is called here unranked_results\n",
    "        # Also have in mind that this function is called from self.query(), so the unranked_results input, \n",
    "        # that in lab1 is called docs, are the results of the query that need to be sorted by importance\n",
    "\n",
    "        # I'm interested only on the element of the docVector corresponding to the query terms\n",
    "        # The remaining elements would become 0 when multiplied to the query_vector\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query)) # I call doc_vectors[k] for a nonexistent key k, the key-value pair (k,[0]*len(terms)) will be automatically added to the dictionary\n",
    "        query_vector = [0] * len(stemmed_query)\n",
    "\n",
    "        # compute the norm for the query tf\n",
    "        query_terms_count = collections.Counter(stemmed_query)  # get the frequency of each term in the query.\n",
    "        # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "\n",
    "        query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "        for termIndex, term in enumerate(stemmed_query):  #termIndex is the index of the term in the query\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "\n",
    "            # TODO: check how to vectorize the query\n",
    "            # query_vector[termIndex]=idf[term]  # original\n",
    "            ## Compute tf*idf(normalize TF as done with documents)\n",
    "            query_vector[termIndex] = query_terms_count[term] / query_norm * self.idf[term]\n",
    "\n",
    "            # Generate doc_vectors for matching docs\n",
    "            for doc_index, (doc, postings) in enumerate(self.index[term]):\n",
    "                # Example of [doc_index, (doc, postings)]\n",
    "                # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "                # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "                # term is in doc 26 in positions 1,4, .....\n",
    "                # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "                #tf[term][0] will contain the tf of the term \"term\" in the doc 26\n",
    "                if doc in unranked_results:\n",
    "                    doc_vectors[doc][termIndex] = self.tf[term][doc_index] * self.idf[term] \n",
    "\n",
    "        # Calculate the score of each doc\n",
    "        # compute the cosine similarity between queyVector and each docVector:\n",
    "        # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "        # see np.dot\n",
    "\n",
    "        doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        print(doc_scores)\n",
    "        self.result_docs = [x[1] for x in doc_scores]\n",
    "        #print document titles instead if document id's\n",
    "        #result_docs=[ title_index[x] for x in result_docs ]\n",
    "        if len(self.result_docs) == 0:\n",
    "            print(\"No results found, try again\")\n",
    "            query = input()\n",
    "            self.query(query)\n",
    "        #print ('\\n'.join(result_docs), '\\n')\n",
    "                                          \n",
    "    def query(self, query):\n",
    "\n",
    "        # TODO: copy the search_tf_idf() function from lab1\n",
    "        # Remember that we dn't give the index as input because we save it in self.index\n",
    "        # Remember that to call what in the lab was rank_documents() we have to call self.rank()\n",
    "        self.ranked_docs = []\n",
    "        query = build_terms(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                docs = term_docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        self.ranked_docs = self.rank(query, docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets in the corpus: 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30 18:39:17+00:00</td>\n",
       "      <td>1575918221013979136</td>\n",
       "      <td>@MelSimmonsFCDO Wrong. Dictator Putin's Fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RussiainvadesUkraine, UkraineRussiaWar]</td>\n",
       "      <td>1404526426330701825</td>\n",
       "      <td>https://twitter.com/1404526426330701825/status...</td>\n",
       "      <td>[MelSimmonsFCDO]</td>\n",
       "      <td>[melsimmonsfcdo, wrong, dictat, putin, fascist...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-30 18:38:44+00:00</td>\n",
       "      <td>1575918081461080064</td>\n",
       "      <td>🇺🇦❤️ The Armed Forces liberated the village of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...</td>\n",
       "      <td>1257116113898536961</td>\n",
       "      <td>https://twitter.com/1257116113898536961/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[arm, forc, liber, villag, drobyshev, lymansk,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30 18:38:23+00:00</td>\n",
       "      <td>1575917992390823936</td>\n",
       "      <td>ALERT 🚨Poland preps anti-radiation tablets ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[NATO, Putin, Russia, RussiaInvadedUkraine, Uk...</td>\n",
       "      <td>1460003892415053828</td>\n",
       "      <td>https://twitter.com/1460003892415053828/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alert, poland, prep, antiradi, tablet, nuclea...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-30 18:38:03+00:00</td>\n",
       "      <td>1575917907774967808</td>\n",
       "      <td>I’m still waiting for my google map 🗺️ to upda...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Putin, UkraineRussiaWar]</td>\n",
       "      <td>285766081</td>\n",
       "      <td>https://twitter.com/285766081/status/157591790...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[im, still, wait, googl, map, updat, russia, n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-30 18:37:56+00:00</td>\n",
       "      <td>1575917878410301440</td>\n",
       "      <td>@EmmanuelMacron probably you're right or you h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[European, UkraineRussiaWar]</td>\n",
       "      <td>1537193346107686915</td>\n",
       "      <td>https://twitter.com/1537193346107686915/status...</td>\n",
       "      <td>[EmmanuelMacron]</td>\n",
       "      <td>[emmanuelmacron, probabl, your, right, say, an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date             tweet_id  \\\n",
       "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
       "1 2022-09-30 18:38:44+00:00  1575918081461080064   \n",
       "2 2022-09-30 18:38:23+00:00  1575917992390823936   \n",
       "3 2022-09-30 18:38:03+00:00  1575917907774967808   \n",
       "4 2022-09-30 18:37:56+00:00  1575917878410301440   \n",
       "\n",
       "                                               tweet  likes  retweets  \\\n",
       "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
       "1  🇺🇦❤️ The Armed Forces liberated the village of...      0         0   \n",
       "2  ALERT 🚨Poland preps anti-radiation tablets ove...      0         0   \n",
       "3  I’m still waiting for my google map 🗺️ to upda...      0         0   \n",
       "4  @EmmanuelMacron probably you're right or you h...      0         0   \n",
       "\n",
       "                                            hashtags              user_id  \\\n",
       "0           [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
       "1  [Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...  1257116113898536961   \n",
       "2  [NATO, Putin, Russia, RussiaInvadedUkraine, Uk...  1460003892415053828   \n",
       "3                          [Putin, UkraineRussiaWar]            285766081   \n",
       "4                       [European, UkraineRussiaWar]  1537193346107686915   \n",
       "\n",
       "                                                 url              tags  \\\n",
       "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
       "1  https://twitter.com/1257116113898536961/status...                []   \n",
       "2  https://twitter.com/1460003892415053828/status...                []   \n",
       "3  https://twitter.com/285766081/status/157591790...                []   \n",
       "4  https://twitter.com/1537193346107686915/status...  [EmmanuelMacron]   \n",
       "\n",
       "                                       stemmed_tweet doc_id  \n",
       "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n",
       "1  [arm, forc, liber, villag, drobyshev, lymansk,...    NaN  \n",
       "2  [alert, poland, prep, antiradi, tablet, nuclea...  doc_3  \n",
       "3  [im, still, wait, googl, map, updat, russia, n...    NaN  \n",
       "4  [emmanuelmacron, probabl, your, right, say, an...    NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_path = '../data/Rus_Ukr_war_data.json'\n",
    "csv_path = '../data/Rus_Ukr_war_data_ids.csv'\n",
    "\n",
    "# Import from JSON file\n",
    "raw_df = from_json_to_dataframe(doc_path)\n",
    "\n",
    "# Clean raw DataFrame to have a more convenient structure\n",
    "clean_df = clean_raw_dataset(raw_df)\n",
    "\n",
    "# \n",
    "clean_df[\"stemmed_tweet\"] = process_text_column(clean_df[\"tweet\"])\n",
    "\n",
    "\n",
    "clean_df = join_docs_tweets_dfs(clean_df, csv_path)\n",
    "\n",
    "\n",
    "print(\"Total number of Tweets in the corpus: {}\".format(len(clean_df)))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = InvertedIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "Ukrain tank\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 97 for the searched query:\n",
      "\n",
      "page_id= 1575914059031101440 - page_title: 🔥🇺🇦🪖 here they demolish some 🇷🇺tanks and a house they were operating out of ~ \n",
      "\n",
      "#UkraineRussiaWar #UkraineWar https://t.co/axJmhsvlUx\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575906144178298880 - page_title: @Den_2042 Strange that these shows don’t reflect what is happening on the battlefield nor what the ongoing mobilization that had to happen to support the losses. Just disconnected from reality. Just like the Nazi regime was when tanks were coming into Germany. Weird. #UkraineRussiaWar\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575891775834972160 - page_title: Russia Loses 2 Jets, 13 Tanks and a Helicopter in a Single Day: Ukraine https://t.co/qVoOoN1dX0 #UkraineWillWin #UkraineRussiaWar #Ukraine #UkraineWar #NATO #RussiaInvadedUkraine #Russians #RussianUkrainianWar #Russia\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575894627009585152 - page_title: Ukrainian forces captured a Russian T-72B3 tank with a KMT-6 mine plough in Donetsk Oblast.\n",
      "#UkraineRussiaWar https://t.co/M7DdWsSFsm\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575893901080027136 - page_title: 🇺🇦 🇷🇺   \n",
      "Russian Tank Crushed By ATGM\n",
      "surprisingly some of the crew survived\n",
      "\n",
      "#Ukraine #Russia #war #putin  #kharkiv #UkraineRussiaWar  #kyiv #NATO https://t.co/GfKWmdkyvV\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575889650471665664 - page_title: Paratroopers of the 95th brigade destroyed a #Russian tank during the assault on #Olhivka in the #Kharkiv region 🔥 #Ukraine️ #Putin #UkraineWar #UkraineRussiaWar #StandWithUkraine #SlavaUkrainii #UkraineWillWin #ukrainecounteroffensive #KharkivOffensive https://t.co/oUr5KtJSwn\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575885452044865536 - page_title: Vladmir #Putin pours in more heavy metal into the raging suicidal war in Ukraine. More Tanks &amp; weapons on the #Crimean bridge rolling straight into #Ukraine. Fasten your seat belts!!\n",
      "\n",
      "#UkraineRussiaWar #Breaking #ukraineinvasion #SlavaUkraïni #bbcnews #Lyman #Kherson #AP #Africa https://t.co/DyIZ8h7kyT\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575876264899923968 - page_title: ❗️Ukraine. Ukrainian military units \"Kraken\" professionally destroyed the tank of Russian terrorists.\n",
      "\n",
      "#StandWithUkraine #UkraineRussiaWar https://t.co/uWOO6j56my\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575875896933752832 - page_title: Avoiding an Aleppo Scenario: This is Why Germany and America Won’t Send Leopard II or Abrams Tanks to Ukraine | Military Watch Magazine https://t.co/CUFNTDySGt \n",
      "\n",
      "#geopolitics #UkraineRussiaWar #UkraineWar #geoeconomics https://t.co/RaLm4HK3sb\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575864537856647168 - page_title: Invaders' tanks with clever attack tactics brought unexpected effects to....  Ukrainian forces 😂🤣🥰\n",
      "#UkraineRussiaWar #UkraineUnderAttack #RussiaIsATerroristState #RussianWarCrimes #HIMARS #StandWithUkraine #ukraine #HIMARSOCLOCK #crimea #donbas #SlavaUkraini #KharkivOblast https://t.co/MlM1STEnzr\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = inverted_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'date'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-b503e573e4a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_tweets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtf_idf_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfIdfIndex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclean_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_tweets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-35631266f2c8>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, lines, num_documents)\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mline_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[0mpage_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m             \u001b[0mterms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_terms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m#page_title + page_text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 10: 'date'"
     ]
    }
   ],
   "source": [
    "# Execute after implementing TFIDF INDEX\n",
    "\n",
    "num_tweets = len(clean_df)\n",
    "tf_idf_index = TfIdfIndex(clean_df, num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also execute after having implemented TFIDF INDEX\n",
    "\n",
    "print(\"Insert your query (i.e.: presidents visiting Kyiv):\\n\")\n",
    "query = input()\n",
    "ranked_docs = tf_idf_index.query(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION PART\n",
    "\n",
    "# We need to import the evaluation_gt file\n",
    "# I think it is a just a csv, so just pandas read csv\n",
    "\n",
    "# We have to do two separate evaluations\n",
    "\n",
    "# First, running the queries in the pdf (they call them information needs)\n",
    "# and then computing the P@K, R@K, etc. for the 3 queries they propose\n",
    "\n",
    "# Second, inventing two new queries, and assessing ourselves if the top N results given by \n",
    "# our algorithm are relevant (1) or not (0), and then computing P@K, R@K, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
