{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_dataframe(doc_path = '../data/Rus_Ukr_war_data.json'):\n",
    "    with open(doc_path) as fp:\n",
    "        lines = fp.readlines()\n",
    "    df=pd.read_json(doc_path, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_dataset(raw_df):\n",
    "    # Select only relevant columns\n",
    "    clean_df = raw_df[[\"created_at\",\"id\",\"full_text\",\"entities\",\"favorite_count\",\"retweet_count\",\"user\"]]\n",
    "\n",
    "    # Rename columns\n",
    "    renames = {\"created_at\":\"date\", \"full_text\":\"tweet\", \"favorite_count\":\"likes\",\"retweet_count\":\"retweets\", \"id\":\"tweet_id\"}\n",
    "    clean_df = clean_df.rename(columns=renames)\n",
    "\n",
    "    # Create Series of list of hashtags from `entities` object\n",
    "    df_hashtags = pd.json_normalize(clean_df[\"entities\"])[\"hashtags\"]\n",
    "    df_hashtags = df_hashtags.apply(lambda x: [item[\"text\"] for item in x])\n",
    "\n",
    "    # Create Series of username ids\n",
    "    df_user = pd.json_normalize(clean_df[\"user\"])[\"id\"].rename(\"user_id\")\n",
    "\n",
    "    # Merge hashtags and username columns to the DataFrame\n",
    "    clean_df = pd.concat([clean_df,df_hashtags,df_user], axis=1).drop(columns=[\"entities\",\"user\"])\n",
    "\n",
    "    # Create URL column manually from the user id and tweet id columns\n",
    "    clean_df[\"url\"] = \"https://twitter.com/\" + clean_df[\"user_id\"].astype(str) + \"/status/\" + clean_df[\"tweet_id\"].astype(str)\n",
    "\n",
    "    # Extract tags to other users from the tweet body\n",
    "    clean_df[\"tags\"] = clean_df[\"tweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "    # Returns a DataFrame of tweets with columns [\"date\", \"tweet_id\", \"tweet\", \"likes\", \"retweets\", \"hashtags\", \"user_id\", \"url\", \"tags\", \"tags\"]\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictograms\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "\n",
    "def clean_tweet(line):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = re.sub(r'[.,;:!?\"\\'-@]', '', line).replace(\"#\", \"\").replace(\"‚Äô\", \"\").replace(\"‚Äú\", \"\").replace(\"\\n\",\" \")\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = remove_emojis(line).strip().replace(\"  \", \" \")\n",
    "    line = line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    line =[word for word in line if word not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line =[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = [word for word in line if word != \"\"]\n",
    "    ## END CODE\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def process_text_column(column):\n",
    "    column = column.apply(clean_tweet)\n",
    "    return column\n",
    "\n",
    "def join_docs_tweets_dfs(tweets, csv_file='../data/Rus_Ukr_war_data_ids.csv'):\n",
    "    docs = pd.read_csv(csv_file, sep=\"\\t\", header=None)\n",
    "    docs = docs.rename(columns={0:\"doc_id\",1:\"tweet_id\"})\n",
    "    tweets = tweets.join(docs.set_index('tweet_id'), on='tweet_id')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "\n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                term_docs = [posting[0] for posting in self.index[term]]\n",
    "                term_docs = set(term_docs)\n",
    "                if len(docs)==0:\n",
    "                    docs = docs.union(term_docs)\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                pass\n",
    "        docs = list(docs)\n",
    "        return docs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TfIdfIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text, num_documents):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = defaultdict(float)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            for term, posting in current_page_index.items():\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "                self.df[term] += 1\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query))\n",
    "        query_vector = [0] * len(stemmed_query)\n",
    "\n",
    "        # compute the norm for the query tf\n",
    "        query_terms_count = collections.Counter(stemmed_query)  # get the frequency of each term in the query.\n",
    "        # Example: collections.Counter([\"hello\",\"hello\",\"world\"]) --> Counter({'hello': 2, 'world': 1})\n",
    "\n",
    "        query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "        for termIndex, term in enumerate(stemmed_query):  #termIndex is the index of the term in the query\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "\n",
    "            # TODO: check how to vectorize the query\n",
    "            # query_vector[termIndex]=idf[term]  # original\n",
    "            ## Compute tf*idf(normalize TF as done with documents)\n",
    "            query_vector[termIndex] = query_terms_count[term] / query_norm * self.idf[term]\n",
    "\n",
    "            # Generate doc_vectors for matching docs\n",
    "            for doc_index, (doc, postings) in enumerate(self.index[term]):\n",
    "                # Example of [doc_index, (doc, postings)]\n",
    "                # 0 (26, array('I', [1, 4, 12, 15, 22, 28, 32, 43, 51, 68, 333, 337]))\n",
    "                # 1 (33, array('I', [26, 33, 57, 71, 87, 104, 109]))\n",
    "                # term is in doc 26 in positions 1,4, .....\n",
    "                # term is in doc 33 in positions 26,33, .....\n",
    "\n",
    "                #tf[term][0] will contain the tf of the term \"term\" in the doc 26\n",
    "                if doc in unranked_results:\n",
    "                    doc_vectors[doc][termIndex] = self.tf[term][doc_index] * self.idf[term] \n",
    "\n",
    "        # Calculate the score of each doc\n",
    "        # compute the cosine similarity between queyVector and each docVector:\n",
    "        # HINT: you can use the dot product because in case of normalized vectors it corresponds to the cosine similarity\n",
    "        # see np.dot\n",
    "\n",
    "        doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        #print(doc_scores)\n",
    "        result_docs = [x[1] for x in doc_scores]\n",
    "        #print document titles instead if document id's\n",
    "        #result_docs=[ title_index[x] for x in result_docs ]\n",
    "\n",
    "        return result_docs\n",
    "                                          \n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                if len(docs)==0:\n",
    "                    docs = docs.union(term_docs)\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        ranked_docs = self.rank(query, docs)\n",
    "\n",
    "        return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets in the corpus: 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30 18:39:17+00:00</td>\n",
       "      <td>1575918221013979136</td>\n",
       "      <td>@MelSimmonsFCDO Wrong. Dictator Putin's Fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RussiainvadesUkraine, UkraineRussiaWar]</td>\n",
       "      <td>1404526426330701825</td>\n",
       "      <td>https://twitter.com/1404526426330701825/status...</td>\n",
       "      <td>[MelSimmonsFCDO]</td>\n",
       "      <td>[melsimmonsfcdo, wrong, dictat, putin, fascist...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-30 18:38:44+00:00</td>\n",
       "      <td>1575918081461080065</td>\n",
       "      <td>üá∫üá¶‚ù§Ô∏è The Armed Forces liberated the village of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...</td>\n",
       "      <td>1257116113898536961</td>\n",
       "      <td>https://twitter.com/1257116113898536961/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[arm, forc, liber, villag, drobyshev, lymansk,...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30 18:38:23+00:00</td>\n",
       "      <td>1575917992390823936</td>\n",
       "      <td>ALERT üö®Poland preps anti-radiation tablets ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[NATO, Putin, Russia, RussiaInvadedUkraine, Uk...</td>\n",
       "      <td>1460003892415053828</td>\n",
       "      <td>https://twitter.com/1460003892415053828/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alert, poland, prep, antiradi, tablet, nuclea...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-30 18:38:03+00:00</td>\n",
       "      <td>1575917907774967809</td>\n",
       "      <td>I‚Äôm still waiting for my google map üó∫Ô∏è to upda...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Putin, UkraineRussiaWar]</td>\n",
       "      <td>285766081</td>\n",
       "      <td>https://twitter.com/285766081/status/157591790...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[im, still, wait, googl, map, updat, russia, n...</td>\n",
       "      <td>doc_4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-30 18:37:56+00:00</td>\n",
       "      <td>1575917878410301441</td>\n",
       "      <td>@EmmanuelMacron probably you're right or you h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[European, UkraineRussiaWar]</td>\n",
       "      <td>1537193346107686915</td>\n",
       "      <td>https://twitter.com/1537193346107686915/status...</td>\n",
       "      <td>[EmmanuelMacron]</td>\n",
       "      <td>[emmanuelmacron, probabl, your, right, say, an...</td>\n",
       "      <td>doc_5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date             tweet_id  \\\n",
       "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
       "1 2022-09-30 18:38:44+00:00  1575918081461080065   \n",
       "2 2022-09-30 18:38:23+00:00  1575917992390823936   \n",
       "3 2022-09-30 18:38:03+00:00  1575917907774967809   \n",
       "4 2022-09-30 18:37:56+00:00  1575917878410301441   \n",
       "\n",
       "                                               tweet  likes  retweets  \\\n",
       "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
       "1  üá∫üá¶‚ù§Ô∏è The Armed Forces liberated the village of...      0         0   \n",
       "2  ALERT üö®Poland preps anti-radiation tablets ove...      0         0   \n",
       "3  I‚Äôm still waiting for my google map üó∫Ô∏è to upda...      0         0   \n",
       "4  @EmmanuelMacron probably you're right or you h...      0         0   \n",
       "\n",
       "                                            hashtags              user_id  \\\n",
       "0           [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
       "1  [Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...  1257116113898536961   \n",
       "2  [NATO, Putin, Russia, RussiaInvadedUkraine, Uk...  1460003892415053828   \n",
       "3                          [Putin, UkraineRussiaWar]            285766081   \n",
       "4                       [European, UkraineRussiaWar]  1537193346107686915   \n",
       "\n",
       "                                                 url              tags  \\\n",
       "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
       "1  https://twitter.com/1257116113898536961/status...                []   \n",
       "2  https://twitter.com/1460003892415053828/status...                []   \n",
       "3  https://twitter.com/285766081/status/157591790...                []   \n",
       "4  https://twitter.com/1537193346107686915/status...  [EmmanuelMacron]   \n",
       "\n",
       "                                       stemmed_tweet doc_id  \n",
       "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n",
       "1  [arm, forc, liber, villag, drobyshev, lymansk,...  doc_2  \n",
       "2  [alert, poland, prep, antiradi, tablet, nuclea...  doc_3  \n",
       "3  [im, still, wait, googl, map, updat, russia, n...  doc_4  \n",
       "4  [emmanuelmacron, probabl, your, right, say, an...  doc_5  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_path = '../data/Rus_Ukr_war_data.json'\n",
    "csv_path = '../data/Rus_Ukr_war_data_ids.csv'\n",
    "\n",
    "# Import from JSON file\n",
    "raw_df = from_json_to_dataframe(doc_path)\n",
    "\n",
    "# Clean raw DataFrame to have a more convenient structure\n",
    "clean_df = clean_raw_dataset(raw_df)\n",
    "\n",
    "# \n",
    "clean_df[\"stemmed_tweet\"] = process_text_column(clean_df[\"tweet\"])\n",
    "\n",
    "\n",
    "clean_df = join_docs_tweets_dfs(clean_df, csv_path)\n",
    "\n",
    "\n",
    "print(\"Total number of Tweets in the corpus: {}\".format(len(clean_df)))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = InvertedIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "ukrain\n",
      "Query: ukrain\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 2605 for the searched query:\n",
      "\n",
      "tweet_id = 1575917751360593920\n",
      "tweet = NATO and Ukraine poke the bear (Russia) then get mad when the bear (Russia) wakes up from the smell of their unsanitary chemical biolabs. #Putin #Ukraine #UkraineWar #UkraineRussiaWar\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575917759707299841\n",
      "tweet = üö®Estonia, Lithuania and Latvia fully support welcoming Ukraine into NATO as soon as possible, Lithuania's Minister of Foreign Affairs says\n",
      "\n",
      "#Russia #RussiaInvadedUkraine #Ukraine #UkraineRussiaWar\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575916494990417920\n",
      "tweet = #Ukraine cannot for the obvious reason that they are at war and no country can join whilst at war. That would also trigger the end of days #WWIII #WW3 #nuclearwar #UkraineWar #UkraineRussiaWar #NATO #EuropeanUnion #news #BreakingNews #BREAKING https://t.co/BqRq91uIX7\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575913989195718657\n",
      "tweet = Former Russian Prime Minister Mikhail Kasyanov believes that Russian President Vladimir #Putin could step down from his position and flee #Russia in a few months.\n",
      "#UkraineÔ∏è #UkraineRussiaWar #NATO #Putler https://t.co/VzzStxdktj\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575913620768456705\n",
      "tweet = #219dayofwar\n",
      "\n",
      "üö®An air raid warning has been issued for all of Ukraine, except occupied #Crimeaüö®\n",
      " \n",
      "#UkraineWar #StandWithUkraine #UkraineRussiaWar #Ukraine #StopRussiaNOW #RussiaIsATerroristState https://t.co/3wSBGBfDnr\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575917748978434053\n",
      "tweet = #UkraineRussiaWar #Ukraine #Russia\n",
      "\n",
      "#OSCE condemns Russia's illegal annexation of Ukrainian territory\n",
      "We once again call on the Russian Federation to withdraw all its forces from across Ukraine\n",
      "https://t.co/hLVEQLjYel https://t.co/kOD7z6KVgE\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575912513090486277\n",
      "tweet = #Ukraine now has capabilities of 92km with new #HIMARS munitions #Ukraine #UkraineWar #UkraineRussiaWar #Russia #russiaisateroriststate https://t.co/pe5zZIQ0ZJ\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575907060897939457\n",
      "tweet = Ukraine is about to encircle Lyman https://t.co/uMebhV4RNv It would be a serious setback for Russia. It could also support Ukrainian attacks on #Lysychansk, one of the largest cities Russia captured early in the invasion | #UkraineRussiaWar üó∫Ô∏è src @TheStudyofWar via @nytgraphics https://t.co/YLO9pDaljX\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575899185202724868\n",
      "tweet = Zelensky's NATO membership bid following Putin's Annexation:\n",
      "NATO Secretary-General Jens Stoltenberg: A decision on Ukraine‚Äôs request to join NATO must be taken by all 30 allies by consensus\n",
      "\n",
      "https://t.co/OSOzYbKCBg\n",
      "#UkraineWar #UkraineKrieg #UkraineRussiaWar  #NATOgoesEast https://t.co/wbZgiysWLc\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575905162195091465\n",
      "tweet = \"Putin's war is an attack on the planet\" \n",
      "Today's New York Times\n",
      "#UkraineRussiaWar #Ukraine #UkraineInvasion https://t.co/SZ1fcnFXDX\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "print(\"Query:\", query)\n",
    "results = inverted_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(results)))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id = {}\\ntweet = {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute after implementing TFIDF INDEX\n",
    "\n",
    "num_tweets = len(clean_df)\n",
    "tf_idf_index = TfIdfIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"], num_documents=num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "spiderman\n",
      "Query: spiderman\n",
      "\n",
      "======================\n",
      "Sample of 1 results out of 10 for the searched query:\n",
      "\n",
      "tweet_id= 1575722686461796352\n",
      "tweet: Spiderman üòÇ\n",
      "#tshirt\n",
      "#shirt\n",
      "#memes\n",
      "#Ukraine \n",
      "#Thunivu \n",
      "#IranProtests2022 \n",
      "#TikTok \n",
      "#UkraineRussiaWar \n",
      "#fun\n",
      "#tshirtdesign https://t.co/nAdiBp2hz1\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Also execute after having implemented TFIDF INDEX\n",
    "\n",
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "print(\"Query:\", query)\n",
    "\n",
    "results = tf_idf_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(len(results), top))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id= {}\\ntweet: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION PART\n",
    "\n",
    "# We need to import the evaluation_gt file\n",
    "# I think it is a just a csv, so just pandas read csv\n",
    "\n",
    "# We have to do two separate evaluations\n",
    "\n",
    "# First, running the queries in the pdf (they call them information needs)\n",
    "# and then computing the P@K, R@K, etc. for the 3 queries they propose\n",
    "\n",
    "# Second, inventing two new queries, and assessing ourselves if the top N results given by \n",
    "# our algorithm are relevant (1) or not (0), and then computing P@K, R@K, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc</th>\n",
       "      <th>query_id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>doc_1452</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>doc_2908</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doc_618</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>doc_489</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>doc_110</td>\n",
       "      <td>Q3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        doc query_id  label\n",
       "0  doc_1452       Q3      1\n",
       "1  doc_2908       Q3      1\n",
       "2   doc_618       Q3      1\n",
       "3   doc_489       Q3      1\n",
       "4   doc_110       Q3      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_df = pd.read_csv(\"../data/Evaluation_gt.csv\")\n",
    "\n",
    "evaluation_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY1 = \"tank Kharkiv\" # What is the discussion regarding a tank in Kharkiv?\n",
    "QUERY2 = \"nord stream\" # What discussion are there about the Nord Stream pipeline?\n",
    "QUERY3 = \"territories annexation russia\" # What is being said about the annexation of territories in Russia?\n",
    "\n",
    "QUERY4 = \"\" # \n",
    "QUER5 = \"putin kill\" # Are there discussions or messages about killing president Putin?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(doc_score, y_score, k = 10): # P@K\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_score: Ground truth (true relevance labels).\n",
    "    y_score: Predicted scores.\n",
    "    k : number of doc to consider.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    precision @k : float\n",
    "\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    relevant = sum(doc_score == 1)\n",
    "    return float(relevant) / k\n",
    "\n",
    "\n",
    "def recall_at_k(doc_score, y_score, k = 10): # R@K\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "    relevant = sum(doc_score == 1)\n",
    "    \n",
    "    doc_score_total = np.take(doc_score, order)\n",
    "    relevant_total = sum(doc_score == 1)\n",
    "    return float(relevant) / len(relevant_total)\n",
    "\n",
    "def avg_precision_at_k(doc_score, y_score, k=10): #Average Precision@K\n",
    "\n",
    "    doc_score = np.array(doc_score)\n",
    "    y_score = np.array(y_score)\n",
    "\n",
    "    gtp = np.sum(doc_score == 1)\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    doc_score = np.take(doc_score, order[:k])\n",
    "\n",
    "    if gtp == 0:\n",
    "        return 0\n",
    "    n_relevant_at_i = 0\n",
    "    prec_at_i = 0\n",
    "    for i in range(len(doc_score)):\n",
    "        if doc_score[i] == 1:\n",
    "            n_relevant_at_i += 1\n",
    "            prec_at_i += n_relevant_at_i / (i + 1)\n",
    "    return prec_at_i / gtp\n",
    "\n",
    "def f1_score(precision, recall): # F1-Score@K\n",
    "\n",
    "    if precision + recall == 0:  # Avoid division by zero\n",
    "        return 0 \n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def mean_avg_precision(avg_precisions): #Mean Average Precision (MAP)\n",
    "\n",
    "    return sum(avg_precisions) / len(avg_precisions)\n",
    "\n",
    "def rr_at_k(doc_score, y_score, k=10):\n",
    "\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    if np.sum(doc_score) == 0:  # if there are not relevant doument return 0\n",
    "        return 0\n",
    "    return 1 / (np.argmax(doc_score == 1) + 1)  # hint: to get the position of the first relevant document use \"np.argmax\"\n",
    "\n",
    "def mean_reciprocal_rank(rr_ranks): # MRR\n",
    "    \n",
    "    return sum(rr_ranks) / len(rr_ranks)\n",
    "\n",
    "\n",
    "def discounted_cumulative_gain(doc_score, y_score, k=10):\n",
    "    order = np.argsort(y_score)[::-1]  # get the list of indexes of the predicted score sorted in descending order.\n",
    "    doc_score = np.take(doc_score, order[:k])  # sort the actual relevance label of the documents based on predicted score(hint: np.take) and take first k.\n",
    "    gain = 2 ** doc_score - 1  # Compute gain (use formula 7 above)\n",
    "    discounts = np.log2(np.arange(len(doc_score)) + 2)  # Compute denominator\n",
    "    return np.sum(gain / discounts)  #return dcg@k\n",
    "\n",
    "\n",
    "def normalized_discounted_cumulative_gain(doc_score, y_score, k=10):\n",
    "    dcg_max = discounted_cumulative_gain(doc_score, doc_score, k)\n",
    "    if not dcg_max:\n",
    "        return 0\n",
    "    return np.round(discounted_cumulative_gain(doc_score, y_score, k) / dcg_max, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       date             tweet_id  \\\n",
      "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
      "\n",
      "                                               tweet  likes  retweets  \\\n",
      "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
      "\n",
      "                                   hashtags              user_id  \\\n",
      "0  [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
      "\n",
      "                                                 url              tags  \\\n",
      "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
      "\n",
      "                                       stemmed_tweet doc_id  \n",
      "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n"
     ]
    }
   ],
   "source": [
    "print(clean_df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PERFORMING QUERY1: tank Kharkiv\n",
      "              tweet_id  true_score\n",
      "0  1575528927245770752           1\n",
      "\n",
      "Using standard Inverted Index...\n",
      "               tweet_id  predicted_score  true_score\n",
      "0   1575482368630353920               27         0.0\n",
      "1   1575889650471665665               26         0.0\n",
      "2   1575196507770593282               25         0.0\n",
      "3   1575204591469150210               24         0.0\n",
      "4   1575458070381092866               23         0.0\n",
      "5   1575263742891552768               22         0.0\n",
      "6   1575893901080027142               21         0.0\n",
      "7   1575215511235096576               20         1.0\n",
      "8   1575204441770594304               19         1.0\n",
      "9   1575642464685170688               18         0.0\n",
      "10  1575462465038872577               17         0.0\n",
      "11  1575187749447307265               16         1.0\n",
      "12  1575822033253867521               15         0.0\n",
      "13  1575600820229242880               14         0.0\n",
      "14  1575697002331246592               13         0.0\n",
      "15  1575457761189679106               12         1.0\n",
      "16  1575528927245770752               11         1.0\n",
      "17  1575446917198540803               10         0.0\n",
      "18  1575180535890325504                9         0.0\n",
      "19  1575610720322211840                8         0.0\n",
      "20  1575481729535463424                7         0.0\n",
      "21  1575482082985660422                6         0.0\n",
      "22  1575834054905462784                5         1.0\n",
      "23  1575546498145984513                4         0.0\n",
      "24  1575435463682363392                3         1.0\n",
      "25  1575739143748927488                2         0.0\n",
      "26  1575642072295489536                1         0.0\n",
      "              tweet_id  predicted_score  true_score\n",
      "0  1575482368630353920               27         0.0\n",
      "1  1575889650471665665               26         0.0\n",
      "2  1575196507770593282               25         0.0\n",
      "3  1575204591469150210               24         0.0\n",
      "4  1575458070381092866               23         0.0 \n",
      "...\n",
      "P@K: 0.0\n",
      "AP@K: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-53-96cd613c0795>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ground_truth_aux[\"label\"] = 0\n"
     ]
    }
   ],
   "source": [
    "print(\"PERFORMING QUERY1:\", QUERY1)\n",
    "\n",
    "queries = [\"Q1\",\"Q2\",\"Q3\"]\n",
    "k = 10\n",
    "avg_precisions = []\n",
    "rr_ranks = []\n",
    "\n",
    "for current_query in queries:\n",
    "\n",
    "    print(f\"--------------------Currently analysing: {current_query} \\n\")\n",
    "#warnings.filterwarnings(\"ignore\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = evaluation_df.loc[evaluation_df[\"query_id\"]==current_query]\n",
    "    ground_truth_aux = evaluation_df.loc[(evaluation_df[\"query_id\"]!=current_query)&(evaluation_df[\"label\"]==1)]\n",
    "    ground_truth_aux[\"label\"] = 0\n",
    "    ground_truth = pd.concat([ground_truth, ground_truth_aux])\n",
    "    ground_truth = ground_truth.merge(clean_df, left_on='doc', right_on='doc_id', how='left')\n",
    "#warnings.filterwarnings(\"default\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = ground_truth[[\"tweet_id\",\"label\"]].rename(columns={\"label\": \"true_score\"})\n",
    "    print(ground_truth.head(1))\n",
    "\n",
    "\n",
    "    print(\"\\nUsing standard Inverted Index...\")\n",
    "    results = inverted_index.search(QUERY1)\n",
    "    results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":reversed(range(1, len(results)+1))})\n",
    "    results = results.merge(ground_truth, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "    # In results we have tweet_ids, predicted_scores and true_score\n",
    "\n",
    "    precision_at_k = precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    avg_precisions.append(precision_at_k)\n",
    "    print(f\"P@K for {current_query} query: {precision_at_k}\")\n",
    "\n",
    "    recall_at_k = recall_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"R@K for {current_query} query: {precision_at_k}\")\n",
    "\n",
    "    avg_precision_at_k = avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"AP@K for {current_query} query: {avg_precision_at_k}\")\n",
    "\n",
    "    f1_score = f1_score(precision_at_k, recall_at_k)\n",
    "    print(f\"F1-Score@K for {current_query} query: {f1_score}\")\n",
    "\n",
    "    rr_at_k = rr_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    rr_ranks.append(rr_at_k)\n",
    "    \n",
    "    ndcg_at_k = normalized_discounted_cumulative_gain(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"NDCG for {current_query} query: {ndcg_at_k}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "mean_avg_precision_at_k = mean_avg_precision(avg_precisions)\n",
    "print(\"MAP:\", mean_avg_precision_at_k)\n",
    "\n",
    "mean_rr_ranks = mean_reciprocal_rank(rr_ranks)\n",
    "print(\"MRR:\", mean_rr_ranks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"\\nUsing TF-IDF Index...\")\n",
    "# results = tf_idf_index.search(QUERY1)\n",
    "# results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":range(1, len(results)+1)})\n",
    "# results = results.merge(ground_truth, on='tweet_id', how='left')\n",
    "# print(results.head(5), \"\\n...\")\n",
    "# # In results we have tweet_ids, predicted_scores and true_score\n",
    "# # TODO: CALL ALL EVALUATION FUNCTIONS AND PRINT THEIR RESULTS\n",
    "# print(\"P@K:\", precision_at_k(results[\"true_score\"], results[\"predicted_score\"]))\n",
    "# print(\"AP@K:\", avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"]))\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "queries = [\"Q1\",\"Q2\",\"Q3\"]\n",
    "k = 10\n",
    "avg_precisions = []\n",
    "rr_ranks = []\n",
    "\n",
    "for current_query in queries:\n",
    "\n",
    "    print(f\"--------------------Currently analysing: {current_query} \\n\")\n",
    "#warnings.filterwarnings(\"ignore\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = evaluation_df.loc[evaluation_df[\"query_id\"]==current_query]\n",
    "    ground_truth_aux = evaluation_df.loc[(evaluation_df[\"query_id\"]!=current_query)&(evaluation_df[\"label\"]==1)]\n",
    "    ground_truth_aux[\"label\"] = 0\n",
    "    ground_truth = pd.concat([ground_truth, ground_truth_aux])\n",
    "    ground_truth = ground_truth.merge(clean_df, left_on='doc', right_on='doc_id', how='left')\n",
    "#warnings.filterwarnings(\"default\", category=pd.core.common.SettingWithCopyWarning)\n",
    "    ground_truth = ground_truth[[\"tweet_id\",\"label\"]].rename(columns={\"label\": \"true_score\"})\n",
    "    print(ground_truth.head(1))\n",
    "\n",
    "\n",
    "    print(\"\\nUsing standard Inverted Index...\")\n",
    "    results = tf_idf_index.search(QUERY1)\n",
    "    results = pd.DataFrame({\"tweet_id\":results, \"predicted_score\":reversed(range(1, len(results)+1))})\n",
    "    results = results.merge(ground_truth, on='tweet_id', how='left').fillna(0)\n",
    "\n",
    "    # In results we have tweet_ids, predicted_scores and true_score\n",
    "\n",
    "    precision_at_k = precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    avg_precisions.append(precision_at_k)\n",
    "    print(f\"P@K for {current_query} query: {precision_at_k}\")\n",
    "\n",
    "    recall_at_k = recall_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"R@K for {current_query} query: {precision_at_k}\")\n",
    "\n",
    "    avg_precision_at_k = avg_precision_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"AP@K for {current_query} query: {avg_precision_at_k}\")\n",
    "\n",
    "    f1_score = f1_score(precision_at_k, recall_at_k)\n",
    "    print(f\"F1-Score@K for {current_query} query: {f1_score}\")\n",
    "\n",
    "    rr_at_k = rr_at_k(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    rr_ranks.append(rr_at_k)\n",
    "    \n",
    "    ndcg_at_k = normalized_discounted_cumulative_gain(results[\"true_score\"], results[\"predicted_score\"])\n",
    "    print(f\"NDCG for {current_query} query: {ndcg_at_k}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "mean_avg_precision_at_k = mean_avg_precision(avg_precisions)\n",
    "print(\"MAP:\", mean_avg_precision_at_k)\n",
    "\n",
    "mean_rr_ranks = mean_reciprocal_rank(rr_ranks)\n",
    "print(\"MRR:\", mean_rr_ranks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
