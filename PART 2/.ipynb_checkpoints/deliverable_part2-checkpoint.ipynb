{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_dataframe(doc_path = '../data/Rus_Ukr_war_data.json'):\n",
    "    with open(doc_path) as fp:\n",
    "        lines = fp.readlines()\n",
    "    df=pd.read_json(doc_path, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_dataset(raw_df):\n",
    "    # Select only relevant columns\n",
    "    clean_df = raw_df[[\"created_at\",\"id_str\",\"full_text\",\"entities\",\"favorite_count\",\"retweet_count\",\"user\"]]\n",
    "\n",
    "    # Rename columns\n",
    "    renames = {\"created_at\":\"date\", \"full_text\":\"tweet\", \"favorite_count\":\"likes\",\"retweet_count\":\"retweets\", \"id_str\":\"tweet_id\"}\n",
    "    clean_df = clean_df.rename(columns=renames)\n",
    "\n",
    "    # Create Series of list of hashtags from `entities` object\n",
    "    df_hashtags = pd.json_normalize(clean_df[\"entities\"])[\"hashtags\"]\n",
    "    df_hashtags = df_hashtags.apply(lambda x: [item[\"text\"] for item in x])\n",
    "\n",
    "    # Create Series of username ids\n",
    "    df_user = pd.json_normalize(clean_df[\"user\"])[\"id\"].rename(\"user_id\")\n",
    "\n",
    "    # Merge hashtags and username columns to the DataFrame\n",
    "    clean_df = pd.concat([clean_df,df_hashtags,df_user], axis=1).drop(columns=[\"entities\",\"user\"])\n",
    "\n",
    "    # Create URL column manually from the user id and tweet id columns\n",
    "    clean_df[\"url\"] = \"https://twitter.com/\" + clean_df[\"user_id\"].astype(str) + \"/status/\" + clean_df[\"tweet_id\"].astype(str)\n",
    "\n",
    "    # Extract tags to other users from the tweet body\n",
    "    clean_df[\"tags\"] = clean_df[\"tweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "    # Returns a DataFrame of tweets with columns [\"date\", \"tweet_id\", \"tweet\", \"likes\", \"retweets\", \"hashtags\", \"user_id\", \"url\", \"tags\", \"tags\"]\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictograms\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "\n",
    "def clean_tweet(line):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = re.sub(r'[.,;:!?\"\\'-@]', '', line).replace(\"#\", \"\").replace(\"’\", \"\").replace(\"“\", \"\").replace(\"\\n\",\" \")\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = remove_emojis(line).strip().replace(\"  \", \" \")\n",
    "    line = line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    line =[word for word in line if word not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line =[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = [word for word in line if word != \"\"]\n",
    "    ## END CODE\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def process_text_column(column):\n",
    "    column = column.apply(clean_tweet)\n",
    "    return column\n",
    "\n",
    "def join_docs_tweets_dfs(tweets, csv_file='../data/Rus_Ukr_war_data_ids.csv'):\n",
    "    docs = pd.read_csv(csv_file, sep=\"\\t\", header=None)\n",
    "    docs = docs.rename(columns={0:\"doc_id\",1:\"tweet_id\"})\n",
    "    tweets = tweets.join(docs.set_index('tweet_id'), on='tweet_id')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1, 3}\n"
     ]
    }
   ],
   "source": [
    "term_docs = {1, 2, 3, 4, 6}\n",
    "b = {1, 3}\n",
    "\n",
    "c = term_docs.intersection(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvertedIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):  # Remember, lines contain all documents from file\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            ## ===============================================================\n",
    "            ## create the index for the current page and store it in current_page_index (current_page_index)\n",
    "            ## current_page_index ==> { ‘term1’: [current_doc, [list of positions]], ...,‘term_n’: [current_doc, [list of positions]]}\n",
    "\n",
    "            ## Example: if the curr_doc has id 1 and its text is \"web retrieval information retrieval\":\n",
    "\n",
    "            ## current_page_index ==> { ‘web’: [1, [0]], ‘retrieval’: [1, [1,4]], ‘information’: [1, [2]]}\n",
    "\n",
    "            ## the term ‘web’ appears in document 1 in positions 0,\n",
    "            ## the term ‘retrieval’ appears in document 1 in positions 1 and 4\n",
    "            ## ===============================================================\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms): # terms contains page_title + page_text. Loop over all terms\n",
    "                try:\n",
    "                    # if the term is already in the index for the current page (current_page_index)\n",
    "                    # append the position to the corresponding list (elemento 1 del arreglo, el 0 es la id del documento)\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]  #'I' indicates unsigned int (int in Python)\n",
    "\n",
    "            # merge the current page index with the main index\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "    def search(self, query):\n",
    "        query = clean_tweet(query) #so that stemed terms are matched in the index\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = [posting[0] for posting in self.index[term]]\n",
    "                # docs = docs Union term_docs\n",
    "                # docs |= set(term_docs)\n",
    "                # MARC: Documents information: Since we are dealing with conjunctive queries (AND),\n",
    "                # each of the returned documents should contain all the words in the query. -> The intersection\n",
    "                term_docs = set(term_docs)\n",
    "                docs = term_docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "        docs = list(docs)\n",
    "        return docs\n",
    "\n",
    "class TfIdfIndex():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "        # TODO: Copy the create_index_tfidf function from lab 1\n",
    "        # You can also use the class InvertedIndex above as reference as to how to code classes in Python\n",
    "        # Have in mind that __init__ should not return anything. The variables that...\n",
    "        # create_index_tfidf returns (index, tf, df, idf) should be stored in the class using:\n",
    "        # self.index, self.tf, etc. We don't need to store title_index, it was just for the lab1\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "        pass\n",
    "        # TODO: Copy the rank_documents function from lab 1\n",
    "        # Have in mind that we don't need to pass index, idf nor tf as they are stored in the class using\n",
    "        # self.index, self.idf, etc.\n",
    "        # Note that terms is called here stemmed query and docs is called here unranked_results\n",
    "        # Also have in mind that this function is called from self.query(), so the unranked_results input, \n",
    "        # that in lab1 is called docs, are the results of the query that need to be sorted by importance\n",
    "\n",
    "    def query(self, query):\n",
    "        pass\n",
    "        # TODO: copy the search_tf_idf() function from lab1\n",
    "        # Remember that we dn't give the index as input because we save it in self.index\n",
    "        # Remember that to call what in the lab was rank_documents() we have to call self.rank()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets in the corpus: 4000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30 18:39:17+00:00</td>\n",
       "      <td>1575918221013979136</td>\n",
       "      <td>@MelSimmonsFCDO Wrong. Dictator Putin's Fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RussiainvadesUkraine, UkraineRussiaWar]</td>\n",
       "      <td>1404526426330701825</td>\n",
       "      <td>https://twitter.com/1404526426330701825/status...</td>\n",
       "      <td>[MelSimmonsFCDO]</td>\n",
       "      <td>[melsimmonsfcdo, wrong, dictat, putin, fascist...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-30 18:38:44+00:00</td>\n",
       "      <td>1575918081461080064</td>\n",
       "      <td>🇺🇦❤️ The Armed Forces liberated the village of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...</td>\n",
       "      <td>1257116113898536961</td>\n",
       "      <td>https://twitter.com/1257116113898536961/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[arm, forc, liber, villag, drobyshev, lymansk,...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30 18:38:23+00:00</td>\n",
       "      <td>1575917992390823936</td>\n",
       "      <td>ALERT 🚨Poland preps anti-radiation tablets ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[NATO, Putin, Russia, RussiaInvadedUkraine, Uk...</td>\n",
       "      <td>1460003892415053828</td>\n",
       "      <td>https://twitter.com/1460003892415053828/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alert, poland, prep, antiradi, tablet, nuclea...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-09-30 18:38:03+00:00</td>\n",
       "      <td>1575917907774967808</td>\n",
       "      <td>I’m still waiting for my google map 🗺️ to upda...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Putin, UkraineRussiaWar]</td>\n",
       "      <td>285766081</td>\n",
       "      <td>https://twitter.com/285766081/status/157591790...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[im, still, wait, googl, map, updat, russia, n...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-09-30 18:37:56+00:00</td>\n",
       "      <td>1575917878410301440</td>\n",
       "      <td>@EmmanuelMacron probably you're right or you h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[European, UkraineRussiaWar]</td>\n",
       "      <td>1537193346107686915</td>\n",
       "      <td>https://twitter.com/1537193346107686915/status...</td>\n",
       "      <td>[EmmanuelMacron]</td>\n",
       "      <td>[emmanuelmacron, probabl, your, right, say, an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date             tweet_id  \\\n",
       "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
       "1 2022-09-30 18:38:44+00:00  1575918081461080064   \n",
       "2 2022-09-30 18:38:23+00:00  1575917992390823936   \n",
       "3 2022-09-30 18:38:03+00:00  1575917907774967808   \n",
       "4 2022-09-30 18:37:56+00:00  1575917878410301440   \n",
       "\n",
       "                                               tweet  likes  retweets  \\\n",
       "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
       "1  🇺🇦❤️ The Armed Forces liberated the village of...      0         0   \n",
       "2  ALERT 🚨Poland preps anti-radiation tablets ove...      0         0   \n",
       "3  I’m still waiting for my google map 🗺️ to upda...      0         0   \n",
       "4  @EmmanuelMacron probably you're right or you h...      0         0   \n",
       "\n",
       "                                            hashtags              user_id  \\\n",
       "0           [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
       "1  [Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...  1257116113898536961   \n",
       "2  [NATO, Putin, Russia, RussiaInvadedUkraine, Uk...  1460003892415053828   \n",
       "3                          [Putin, UkraineRussiaWar]            285766081   \n",
       "4                       [European, UkraineRussiaWar]  1537193346107686915   \n",
       "\n",
       "                                                 url              tags  \\\n",
       "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
       "1  https://twitter.com/1257116113898536961/status...                []   \n",
       "2  https://twitter.com/1460003892415053828/status...                []   \n",
       "3  https://twitter.com/285766081/status/157591790...                []   \n",
       "4  https://twitter.com/1537193346107686915/status...  [EmmanuelMacron]   \n",
       "\n",
       "                                       stemmed_tweet doc_id  \n",
       "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n",
       "1  [arm, forc, liber, villag, drobyshev, lymansk,...    NaN  \n",
       "2  [alert, poland, prep, antiradi, tablet, nuclea...  doc_3  \n",
       "3  [im, still, wait, googl, map, updat, russia, n...    NaN  \n",
       "4  [emmanuelmacron, probabl, your, right, say, an...    NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_path = '../data/Rus_Ukr_war_data.json'\n",
    "csv_path = '../data/Rus_Ukr_war_data_ids.csv'\n",
    "\n",
    "# Import from JSON file\n",
    "raw_df = from_json_to_dataframe(doc_path)\n",
    "\n",
    "# Clean raw DataFrame to have a more convenient structure\n",
    "clean_df = clean_raw_dataset(raw_df)\n",
    "\n",
    "# \n",
    "clean_df[\"stemmed_tweet\"] = process_text_column(clean_df[\"tweet\"])\n",
    "\n",
    "\n",
    "clean_df = join_docs_tweets_dfs(clean_df, csv_path)\n",
    "\n",
    "\n",
    "print(\"Total number of Tweets in the corpus: {}\".format(len(clean_df)))\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = InvertedIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 530 for the searched query:\n",
      "\n",
      "page_id= 1575915581278420992 - page_title: Russian bombers capable of carrying nukes detected near #Finland\n",
      "The bombers, capable of carrying cruise missiles and strategic nuclear weapons\n",
      "#Ukraine-#UkraineRussianWar -#UkraineWar -#UkraineRussiaWar\n",
      "https://t.co/LxV4dGSpfW\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575908722077229056 - page_title: Big #BreakingNews | Russian bombers capable of carrying nukes detected near Finland\n",
      "\n",
      "#RussianMobilization #RussianBombers #Finland #UkraineRussiaWar #nuclearweapons \n",
      "\n",
      "https://t.co/aLPdcrNWdX\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575821705695535104 - page_title: #Russia #Ukraine \n",
      "NATO: How Finland will fight in a war with Russia after Ukraine invasion\n",
      "#Finland accepted into #NATO\n",
      "\n",
      "#Putin #Russian #RussianArmy #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA\n",
      "https://t.co/vBtrF8XfGL\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575826730132340736 - page_title: 🛡️ Interesting comparison. In terms of population, this is also appalling. The four regions have around 9M people which is equivalent of Finland and the three Baltic countries combined🇫🇮🇱🇻🇪🇪🇱🇹\n",
      "https://t.co/M24SdldblU\n",
      "\n",
      "#WarInUkraine #UkraineRussiaWar #UkraineWar #Ukraine https://t.co/2e5zu46sAL https://t.co/s7QKHsMAP1\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575754311711064064 - page_title: How Stupid Can Wars Get? Sabotaging Gas Lines\n",
      "This153 Day of #1001DaysToEndWars \n",
      "#Germany #EU #Russia #ukraine #netherlands #youth #cop27 #ClimateAction #ClimateCrisis #finlandia #Norway #Iceland #British #NorwayUNSC #finlande #Denmark #Sweden #UkraineRussiaWar #UNSC #UNGA #war https://t.co/Egx2wY8f1n\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575748908256358400 - page_title: Advisories incldg. U.S. govt: Foreign citizens advised to leave Russia immediately. Do not delay. #Ukraine #UkraineWar #UkraineRussiaWar #Russian #Deutschland #Finland #Gasumlage https://t.co/WOGomdjDij\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575692255612248064 - page_title: Myth \"#Russians are afraid of #NATO expansion\"  \n",
      "\n",
      "#Russia is withdrawing almost all its troops from borders of NATO countries and #Finland to deploy them in #UkraineRussiaWar \n",
      "\n",
      "#Kremlin isn't afraid of NATO attacking, its concern is not being able to attack and annex #Ukraine️\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575645549633495040 - page_title: Zhukov marched infantry through minefields to clear them. Stalin lost 120k in 3 months in Finland. All those stolen territories are cannon fodder warehouses by design. russia never cared about troop losses. Putler would nuke his own cannon fodder\n",
      "#Kherson #UkraineRussiawar #Lyman\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575642018562203648 - page_title: #Russia #Ukraine \n",
      "NATO: How Finland will fight in a war with Russia after Ukraine invasion\n",
      "#Finland accepted into #NATO\n",
      "\n",
      "#Putin #Russian #RussianArmy #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA\n",
      "https://t.co/vBtrF8XfGL\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "page_id= 1575631627404816384 - page_title: Listening to this banger of a song right now. I am glad I will be able to call Finland our brothers in arms. 🫡🇨🇿🫡🇫🇮🫡🇺🇦🫡🇪🇺 #NATO #UkraineRussiaWar #RussiaInvadedUkraine \n",
      "https://t.co/OuoVW6LmvE\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = input()\n",
    "docs = inverted_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute after implementing TFIDF INDEX\n",
    "\n",
    "num_tweets = len(clean_df)\n",
    "tf_idf_index = TfIdfIndex(clean_df, num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also execute after having implemented TFIDF INDEX\n",
    "\n",
    "print(\"Insert your query (i.e.: presidents visiting Kyiv):\\n\")\n",
    "query = input()\n",
    "ranked_docs = tf_idf_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(docs)))\n",
    "for d_id in docs[:top]:\n",
    "    print(\"page_id= {} - page_title: {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EVALUATION PART\n",
    "\n",
    "# We need to import the evaluation_gt file\n",
    "# I think it is a just a csv, so just pandas read csv\n",
    "\n",
    "# We have to do two separate evaluations\n",
    "\n",
    "# First, running the queries in the pdf (they call them information needs)\n",
    "# and then computing the P@K, R@K, etc. for the 3 queries they propose\n",
    "\n",
    "# Second, inventing two new queries, and assessing ourselves if the top N results given by \n",
    "# our algorithm are relevant (1) or not (0), and then computing P@K, R@K, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
