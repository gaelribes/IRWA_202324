{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from array import array\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "import numpy as np\n",
    "import collections\n",
    "from numpy import linalg as la\n",
    "import time\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the next lines to run on Google Colab, please.\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOC_PATH = '../data/Rus_Ukr_war_data.json'\n",
    "MAPPING_PATH = '../data/Rus_Ukr_war_data_ids.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions and modules\n",
    "\n",
    "### Data cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_json_to_dataframe(doc_path = '../data/Rus_Ukr_war_data.json'):\n",
    "    #with open(doc_path) as fp:\n",
    "    #    lines = fp.readlines()\n",
    "    df=pd.read_json(doc_path, lines=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_raw_dataset(raw_df):\n",
    "    # Select only relevant columns\n",
    "    clean_df = raw_df[[\"created_at\",\"id\",\"full_text\",\"entities\",\"favorite_count\",\"retweet_count\",\"user\"]]\n",
    "\n",
    "    # Rename columns\n",
    "    renames = {\"created_at\":\"date\", \"full_text\":\"tweet\", \"favorite_count\":\"likes\",\"retweet_count\":\"retweets\", \"id\":\"tweet_id\"}\n",
    "    clean_df = clean_df.rename(columns=renames)\n",
    "\n",
    "    # Create Series of list of hashtags from `entities` object\n",
    "    df_hashtags = pd.json_normalize(clean_df[\"entities\"])[\"hashtags\"]\n",
    "    df_hashtags = df_hashtags.apply(lambda x: [item[\"text\"] for item in x])\n",
    "\n",
    "    # Create Series of username ids\n",
    "    df_user = pd.json_normalize(clean_df[\"user\"])[\"id\"].rename(\"user_id\")\n",
    "\n",
    "    # Merge hashtags and username columns to the DataFrame\n",
    "    clean_df = pd.concat([clean_df,df_hashtags,df_user], axis=1).drop(columns=[\"entities\",\"user\"])\n",
    "\n",
    "    # Create URL column manually from the user id and tweet id columns\n",
    "    clean_df[\"url\"] = \"https://twitter.com/\" + clean_df[\"user_id\"].astype(str) + \"/status/\" + clean_df[\"tweet_id\"].astype(str)\n",
    "\n",
    "    # Extract tags to other users from the tweet body\n",
    "    clean_df[\"tags\"] = clean_df[\"tweet\"].apply(lambda x: re.findall(r\"@(\\w+)\", x))\n",
    "\n",
    "    # Returns a DataFrame of tweets with columns [\"date\", \"tweet_id\", \"tweet\", \"likes\", \"retweets\", \"hashtags\", \"user_id\", \"url\", \"tags\", \"tags\"]\n",
    "    return clean_df\n",
    "\n",
    "\n",
    "def remove_emojis(tweet):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emojis\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictograms\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "\n",
    "def clean_tweet(line):\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    ## START CODE\n",
    "    line = re.sub(r'[.,;:!?\"\\'-@]', '', line).replace(\"#\", \"\").replace(\"‚Äô\", \"\").replace(\"‚Äú\", \"\").replace(\"\\n\",\" \")\n",
    "    line =  line.lower() ## Transform in lowercase\n",
    "    line = remove_emojis(line).strip().replace(\"  \", \" \")\n",
    "    line = line.split(\" \") ## Tokenize the text to get a list of terms\n",
    "    line =[word for word in line if word not in stop_words]  ## eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line =[stemmer.stem(word) for word in line] ## perform stemming (HINT: use List Comprehension)\n",
    "    line = [word for word in line if word != \"\"]\n",
    "    ## END CODE\n",
    "    \n",
    "    return line\n",
    "\n",
    "\n",
    "def process_text_column(column):\n",
    "    column = column.apply(clean_tweet)\n",
    "    return column\n",
    "\n",
    "def join_docs_tweets_dfs(tweets, csv_file='../data/Rus_Ukr_war_data_ids.csv'):\n",
    "    docs = pd.read_csv(csv_file, sep=\"\\t\", header=None)\n",
    "    docs = docs.rename(columns={0:\"doc_id\",1:\"tweet_id\"})\n",
    "    tweets = tweets.join(docs.set_index('tweet_id'), on='tweet_id')\n",
    "    return tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search engine models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfIdfIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text, num_documents):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = defaultdict(float)\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            for term, posting in current_page_index.items():\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "                self.df[term] += 1\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query))\n",
    "        query_vector = [0] * len(stemmed_query)\n",
    "\n",
    "        query_terms_count = collections.Counter(stemmed_query)\n",
    "\n",
    "        query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "        for termIndex, term in enumerate(stemmed_query):\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "            query_vector[termIndex] = query_terms_count[term] / query_norm * self.idf[term]\n",
    "\n",
    "            for doc_index, (doc, postings) in enumerate(self.index[term]):\n",
    "\n",
    "                if doc in unranked_results:\n",
    "                    doc_vectors[doc][termIndex] = self.tf[term][doc_index] * self.idf[term] \n",
    "\n",
    "        doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "        return result_docs\n",
    "                                          \n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                if len(docs)==0:\n",
    "                    docs = term_docs\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "            \n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        ranked_docs = self.rank(query, docs)\n",
    "\n",
    "        return ranked_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our score will be computed as follows:\n",
    "\n",
    "$$\n",
    "\\text{Our score} = \\frac{1}{4} \\text{likes} + \\frac{3}{4} \\text{retweets}\n",
    "$$\n",
    "\n",
    "Where $\\text{likes}$ and $\\text{retweets}$ are normalized from $0$ to $1$, and in logarithmic scale, as in the EDA in Part 1 we saw that there were a lot of tweets with few likes and retweets and very few tweets with a lot of likes and retweets.\n",
    "\n",
    "We gave different weights to the number of likes and the number of retweets because we considered that, in Twitter, retweets are more representative of the popularity of a tweet because when retweeting a tweet, it appears in your feed so that your community also interacts with the tweet. On the other hand, liking a tweet only represents the fact that you agree or enjoy the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurScore():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text, num_documents, our_score, alpha=0.5):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = defaultdict(float)\n",
    "        self.our_score = our_score\n",
    "        self.alpha = alpha\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            for term, posting in current_page_index.items():\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "                self.df[term] += 1\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query))\n",
    "        query_vector = [0] * len(stemmed_query)\n",
    "\n",
    "        query_terms_count = collections.Counter(stemmed_query)\n",
    "\n",
    "        query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "        for termIndex, term in enumerate(stemmed_query):\n",
    "            if term not in self.index:\n",
    "                continue\n",
    "            query_vector[termIndex] = query_terms_count[term] / query_norm * self.idf[term]\n",
    "\n",
    "            for doc_index, (doc, postings) in enumerate(self.index[term]):\n",
    "\n",
    "                if doc in unranked_results:\n",
    "                    doc_vectors[doc][termIndex] = self.tf[term][doc_index] * self.idf[term] \n",
    "\n",
    "        doc_scores = [[(1-self.alpha) * np.dot(curDocVec, query_vector) + self.alpha * self.our_score[self.our_score[\"tweet_id\"]==doc][\"score\"].item(), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "        return result_docs\n",
    "                                          \n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                if len(docs)==0:\n",
    "                    docs = term_docs\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "            \n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        ranked_docs = self.rank(query, docs)\n",
    "\n",
    "        return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_our_score (clean_df):\n",
    "\n",
    "    tweet_ids = clean_df[\"tweet_id\"]\n",
    "\n",
    "    # LOG-SCALE AND NORMALIZE 0-1\n",
    "    likes = np.log(clean_df[\"likes\"].apply(lambda x: x + 1))\n",
    "    likes = (likes - np.min(likes)) / (np.max(likes) - np.min(likes))\n",
    "\n",
    "    retweets = np.log(clean_df[\"retweets\"].apply(lambda x: x + 1))\n",
    "    retweets = (retweets - np.min(retweets)) / (np.max(retweets) - np.min(retweets))\n",
    "\n",
    "    # COMPUTE USING OUR FORMULA\n",
    "    our_score = likes.apply(lambda x: x*0.25) + likes.apply(lambda x: x*0.75)\n",
    "\n",
    "    # RETURN DATAFRAME OF TWEET IDS AND SCORES\n",
    "    return pd.DataFrame({\"tweet_id\": tweet_ids, \"score\": our_score})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of Tweets in the corpus: 4000\n"
     ]
    }
   ],
   "source": [
    "# Import from JSON file\n",
    "raw_df = from_json_to_dataframe(DOC_PATH)\n",
    "\n",
    "# Clean raw DataFrame to have a more convenient structure\n",
    "clean_df = clean_raw_dataset(raw_df)\n",
    "\n",
    "# Stem tweets\n",
    "clean_df[\"stemmed_tweet\"] = process_text_column(clean_df[\"tweet\"])\n",
    "\n",
    "# Join with map csv\n",
    "clean_df = join_docs_tweets_dfs(clean_df, MAPPING_PATH)\n",
    "\n",
    "\n",
    "print(\"Total number of Tweets in the corpus: {}\".format(len(clean_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>likes</th>\n",
       "      <th>retweets</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>user_id</th>\n",
       "      <th>url</th>\n",
       "      <th>tags</th>\n",
       "      <th>stemmed_tweet</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-09-30 18:39:17+00:00</td>\n",
       "      <td>1575918221013979136</td>\n",
       "      <td>@MelSimmonsFCDO Wrong. Dictator Putin's Fascis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[RussiainvadesUkraine, UkraineRussiaWar]</td>\n",
       "      <td>1404526426330701825</td>\n",
       "      <td>https://twitter.com/1404526426330701825/status...</td>\n",
       "      <td>[MelSimmonsFCDO]</td>\n",
       "      <td>[melsimmonsfcdo, wrong, dictat, putin, fascist...</td>\n",
       "      <td>doc_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-09-30 18:38:44+00:00</td>\n",
       "      <td>1575918081461080065</td>\n",
       "      <td>üá∫üá¶‚ù§Ô∏è The Armed Forces liberated the village of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...</td>\n",
       "      <td>1257116113898536961</td>\n",
       "      <td>https://twitter.com/1257116113898536961/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[arm, forc, liber, villag, drobyshev, lymansk,...</td>\n",
       "      <td>doc_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-09-30 18:38:23+00:00</td>\n",
       "      <td>1575917992390823936</td>\n",
       "      <td>ALERT üö®Poland preps anti-radiation tablets ove...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[NATO, Putin, Russia, RussiaInvadedUkraine, Uk...</td>\n",
       "      <td>1460003892415053828</td>\n",
       "      <td>https://twitter.com/1460003892415053828/status...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[alert, poland, prep, antiradi, tablet, nuclea...</td>\n",
       "      <td>doc_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date             tweet_id  \\\n",
       "0 2022-09-30 18:39:17+00:00  1575918221013979136   \n",
       "1 2022-09-30 18:38:44+00:00  1575918081461080065   \n",
       "2 2022-09-30 18:38:23+00:00  1575917992390823936   \n",
       "\n",
       "                                               tweet  likes  retweets  \\\n",
       "0  @MelSimmonsFCDO Wrong. Dictator Putin's Fascis...      0         0   \n",
       "1  üá∫üá¶‚ù§Ô∏è The Armed Forces liberated the village of...      0         0   \n",
       "2  ALERT üö®Poland preps anti-radiation tablets ove...      0         0   \n",
       "\n",
       "                                            hashtags              user_id  \\\n",
       "0           [RussiainvadesUkraine, UkraineRussiaWar]  1404526426330701825   \n",
       "1  [Drobysheve, Lymansk, Donetsk, UkraineRussiaWa...  1257116113898536961   \n",
       "2  [NATO, Putin, Russia, RussiaInvadedUkraine, Uk...  1460003892415053828   \n",
       "\n",
       "                                                 url              tags  \\\n",
       "0  https://twitter.com/1404526426330701825/status...  [MelSimmonsFCDO]   \n",
       "1  https://twitter.com/1257116113898536961/status...                []   \n",
       "2  https://twitter.com/1460003892415053828/status...                []   \n",
       "\n",
       "                                       stemmed_tweet doc_id  \n",
       "0  [melsimmonsfcdo, wrong, dictat, putin, fascist...  doc_1  \n",
       "1  [arm, forc, liber, villag, drobyshev, lymansk,...  doc_2  \n",
       "2  [alert, poland, prep, antiradi, tablet, nuclea...  doc_3  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype              \n",
      "---  ------         --------------  -----              \n",
      " 0   date           4000 non-null   datetime64[ns, UTC]\n",
      " 1   tweet_id       4000 non-null   int64              \n",
      " 2   tweet          4000 non-null   object             \n",
      " 3   likes          4000 non-null   int64              \n",
      " 4   retweets       4000 non-null   int64              \n",
      " 5   hashtags       4000 non-null   object             \n",
      " 6   user_id        4000 non-null   int64              \n",
      " 7   url            4000 non-null   object             \n",
      " 8   tags           4000 non-null   object             \n",
      " 9   stemmed_tweet  4000 non-null   object             \n",
      " 10  doc_id         4000 non-null   object             \n",
      "dtypes: datetime64[ns, UTC](1), int64(4), object(6)\n",
      "memory usage: 343.9+ KB\n"
     ]
    }
   ],
   "source": [
    "clean_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Comparing TFIDF+Cos vs. TFIDF+OurScore+Cos ranking methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_index = TfIdfIndex(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"], num_documents=len(clean_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "our_score = compute_our_score(clean_df)\n",
    "our_score_index = OurScore(ids=clean_df[\"tweet_id\"], stemmed_text = clean_df[\"stemmed_tweet\"], num_documents=len(clean_df), our_score=our_score, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tank Kharkiv\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 27 for the searched query:\n",
      "\n",
      "tweet_id = 1575739143748927488\n",
      "tweet = Ukrainian tank holds the ground against two advancing russian tanks.\n",
      "\n",
      "Unfortunately, the Ukrainian tank takes a fatal hit\n",
      "\n",
      "#Ukraine #UkraineRussiaCrisis #WarCrimes #UkraineRussiaWar #Kyiv #Mariupol #Chernihiv #Lviv #Kharkiv #Melitopol #Irpin #Bucha #Borodyanka #Odesa #Crimea https://t.co/cYGjgmoeLP\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575528927245770752\n",
      "tweet = Destroyed Ukrainian tank in the Kharkiv region.\n",
      "\n",
      "#Ukraine #Ukrainewar #UkraineRussiaWar #Kharkiv https://t.co/xt4JVrWchP\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575187749447307265\n",
      "tweet = Soldiers of the Ukrainian army seizing a Russian tank in the #Kharkiv region \n",
      "\n",
      "#Ukraine-#UkraineRussianWar -#UkraineWar -#UkraineRussiaWar-#Russian https://t.co/7zqjb8cYCE\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575435463682363392\n",
      "tweet = In the Kharkiv region, the air reconnaissance of the State Border Service eliminated a tank, a Tigr armored vehicle and two trucks\n",
      "#UkraineRussiaWar https://t.co/T5j1LSXGDt\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575196507770593282\n",
      "tweet = Another 2 abandoned T-80BV tanks were found in #Kharkiv Oblast! But it was a \"scheduled exit without equipment\" ü§£ü§£ü§£\n",
      "\n",
      " #RussiaUkraineWar #Russia #UkraineRussianWar #UkraineRussiaWar #UkraineWar #UkrainianArmy #Ukraine #Kherson #Donbass #odessa #Kyiv #Kharkiv #NATO #USA https://t.co/rzR3UM16Xn\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575893901080027142\n",
      "tweet = üá∫üá¶ üá∑üá∫   \n",
      "Russian Tank Crushed By ATGM\n",
      "surprisingly some of the crew survived\n",
      "\n",
      "#Ukraine #Russia #war #putin  #kharkiv #UkraineRussiaWar  #kyiv #NATO https://t.co/GfKWmdkyvV\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575610720322211840\n",
      "tweet = #RussianArmy #Ukraine #Kharkiv  #UkraineRussiaWar #PutinsWar #UkraineWar  #Putin #Donetsk #RussiaInvadedUkraine #KhersonisUkraine   #UkraineWillWin #PutinWarCriminal \n",
      "Ukrainian soldiers pose in front of a destroyed Russian Army T-72B3 main battle tank, in Kharkiv oblast. https://t.co/nwtiBkVKM2\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575600820229242880\n",
      "tweet = #RussianArmy #Ukraine #Kharkiv  #UkraineRussiaWar #PutinsWar #UkraineWar  #Putin #Donetsk #RussiaInvadedUkraine   #UkraineWillWin #PutinWarCriminal \n",
      "Ukrainian forces find two more abandoned Russian Army main battle tanks in Kharkiv oblast.\n",
      "One is a T-80BV, the other a T-80U https://t.co/4kV8Gu7ZaU\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575834054905462784\n",
      "tweet = üëèFootage of a Russian tank being knocked out by Ukrainian forces in Ol'hivka, Kharkiv Oblast.\n",
      "#Russian #Russia #Ukraine #Ukrainian #UkraineWar #UkraineRussiaWar #RussiaIsATerroristStateR https://t.co/YMADdnvXWQ\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575889650471665665\n",
      "tweet = Paratroopers of the 95th brigade destroyed a #Russian tank during the assault on #Olhivka in the #Kharkiv region üî• #UkraineÔ∏è #Putin #UkraineWar #UkraineRussiaWar #StandWithUkraine #SlavaUkrainii #UkraineWillWin #ukrainecounteroffensive #KharkivOffensive https://t.co/oUr5KtJSwn\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"tank Kharkiv\"\n",
    "print(\"Query:\", query)\n",
    "results = our_score_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(results)))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id = {}\\ntweet = {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert your query:\n",
      "\n",
      "TFIDF result: 1575739143748927488, OURSCORE result: 1575739143748927488\n",
      "TFIDF result: 1575528927245770752, OURSCORE result: 1575528927245770752\n",
      "TFIDF result: 1575187749447307265, OURSCORE result: 1575187749447307265\n",
      "TFIDF result: 1575893901080027142, OURSCORE result: 1575435463682363392\n",
      "TFIDF result: 1575435463682363392, OURSCORE result: 1575196507770593282\n",
      "TFIDF result: 1575610720322211840, OURSCORE result: 1575893901080027142\n",
      "TFIDF result: 1575600820229242880, OURSCORE result: 1575610720322211840\n",
      "TFIDF result: 1575196507770593282, OURSCORE result: 1575600820229242880\n",
      "TFIDF result: 1575889650471665665, OURSCORE result: 1575834054905462784\n",
      "TFIDF result: 1575834054905462784, OURSCORE result: 1575889650471665665\n",
      "There are 3/10  tweets in common using the two methods.\n"
     ]
    }
   ],
   "source": [
    "print(\"Insert your query:\\n\")\n",
    "query = \"tank Kharkiv\"\n",
    "\n",
    "results_tfidf = tfidf_index.search(query)\n",
    "results_ourscore = our_score_index.search(query)\n",
    "\n",
    "k = 10\n",
    "\n",
    "matches = 0\n",
    "\n",
    "for i in range(min(len(results_tfidf), k)):\n",
    "    print(f\"TFIDF result: {results_tfidf[i]}, OURSCORE result: {results_ourscore[i]}\")\n",
    "    if results_tfidf[i] == results_ourscore[i]:\n",
    "        matches += 1\n",
    "\n",
    "print(\"There are\", str(matches)+\"/\"+str(k), \" tweets in common using the two methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Word2Vec + cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY1 = \"tank Kharkiv\" # What is the discussion regarding a tank in Kharkiv?\n",
    "QUERY2 = \"nord stream\" # What discussion are there about the Nord Stream pipeline?\n",
    "QUERY3 = \"territories annexation russia\" # What is being said about the annexation of territories in Russia?\n",
    "\n",
    "QUERY4 = \"refugees\" # Are there discussions about the Ukranian refugees?\n",
    "QUERY5 = \"kill putin\" # Are there discussions or messages about killing president Putin or Putin killing people?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Word2VecIndex():\n",
    "\n",
    "    def __init__(self, ids, stemmed_text, num_documents, word2vec_model):\n",
    "        \n",
    "        self.index = defaultdict(list)\n",
    "        self.tf = defaultdict(list)\n",
    "        self.df = defaultdict(int)\n",
    "        self.idf = defaultdict(float)\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.collection = dict(zip(ids, stemmed_text))\n",
    "\n",
    "        stemmed_text = stemmed_text.tolist()\n",
    "        ids = ids.tolist()\n",
    "\n",
    "        for i in range(len(ids)):\n",
    "\n",
    "            tweet = stemmed_text[i]\n",
    "            tweet_id = ids[i]\n",
    "\n",
    "            terms = [word for word in tweet]\n",
    "            page_id = int(tweet_id)\n",
    "\n",
    "            current_page_index = {}\n",
    "\n",
    "            for position, term in enumerate(terms):\n",
    "                try:\n",
    "                    current_page_index[term][1].append(position)\n",
    "                except:\n",
    "                    current_page_index[term] = [page_id, array('I', [position])]\n",
    "\n",
    "            norm = 0\n",
    "            for term, posting in current_page_index.items():\n",
    "                norm += len(posting[1]) ** 2\n",
    "            norm = math.sqrt(norm)\n",
    "\n",
    "            for term, posting in current_page_index.items():\n",
    "                self.tf[term].append(np.round(len(posting[1]) / norm, 4))\n",
    "                self.df[term] += 1\n",
    "\n",
    "            for term_page, posting_page in current_page_index.items():\n",
    "                self.index[term_page].append(posting_page)\n",
    "\n",
    "            for term in self.df:\n",
    "                self.idf[term] = np.round(np.log(float(num_documents / self.df[term])), 4)\n",
    "\n",
    "    def model(self, text):\n",
    "        \n",
    "        tweet_vectors = []\n",
    "\n",
    "        for word in text:\n",
    "            tweet_vectors.append(self.word2vec_model.wv[word])\n",
    "\n",
    "        return np.average(tweet_vectors, axis=0) # Computes the average of all the tweet vectors\n",
    "\n",
    "\n",
    "    def rank(self, stemmed_query, unranked_results):\n",
    "                                          \n",
    "        doc_vectors = defaultdict(lambda: [0] * len(stemmed_query))\n",
    "        query_vector = self.model(stemmed_query)\n",
    "\n",
    "        for doc in unranked_results:\n",
    "            doc_vectors[doc] = self.model(self.collection[doc])\n",
    "\n",
    "        doc_scores = [[np.dot(curDocVec, query_vector), doc] for doc, curDocVec in doc_vectors.items()]\n",
    "        doc_scores.sort(reverse=True)\n",
    "        result_docs = [x[1] for x in doc_scores]\n",
    "\n",
    "        return result_docs\n",
    "                                          \n",
    "    def search(self, query):\n",
    "\n",
    "        query = clean_tweet(query)\n",
    "        docs = set()\n",
    "        for term in query:\n",
    "            try:\n",
    "                # store in term_docs the ids of the docs that contain \"term\"\n",
    "                term_docs = set([posting[0] for posting in self.index[term]])\n",
    "                                          \n",
    "                # retain all documents which contain all words from the query\n",
    "                if len(docs)==0:\n",
    "                    docs = term_docs\n",
    "                else:\n",
    "                    docs = docs.intersection(term_docs)\n",
    "            except:\n",
    "                #term is not in index\n",
    "                pass\n",
    "            \n",
    "        docs = list(docs) #docs are the unranked results\n",
    "                                          \n",
    "        ranked_docs = self.rank(query, docs)\n",
    "\n",
    "        return ranked_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 300\n",
    "\n",
    "model = Word2Vec(sentences=clean_df[\"stemmed_tweet\"], vector_size=vector_size, window=5, min_count=1, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_score_index = Word2VecIndex(ids=clean_df[\"tweet_id\"], stemmed_text=clean_df[\"stemmed_tweet\"], num_documents=len(clean_df), word2vec_model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: tank Kharkiv\n",
      "\n",
      "======================\n",
      "Sample of 10 results out of 27 for the searched query:\n",
      "\n",
      "tweet_id = 1575528927245770752\n",
      "tweet = Destroyed Ukrainian tank in the Kharkiv region.\n",
      "\n",
      "#Ukraine #Ukrainewar #UkraineRussiaWar #Kharkiv https://t.co/xt4JVrWchP\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575642072295489536\n",
      "tweet = #Russia #Ukraine #RussianArmy \n",
      "Why the Russian ü™ñ Army T-72 Tank is Worse Than You Think\n",
      "\n",
      "#Putin #Russian #RussiaUkraineWar #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA #Russians #NATO\n",
      "https://t.co/XZU7FObkVS\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575482368630353920\n",
      "tweet = \"#Russia #Ukraine #RussianArmy \n",
      "Why the Russian ü™ñ Army T-72 Tank is Worse Than You Think\n",
      "\n",
      "#Putin #Russian #RussiaUkraineWar #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA #Russians #NATO\n",
      "https://t.co/XZU7FNUhTS\"\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575457761189679106\n",
      "tweet = #Russia #Ukraine #RussianArmy \n",
      "Why the Russian ü™ñ Army T-72 Tank is Worse Than You Think\n",
      "\n",
      "#Putin #Russian #RussiaUkraineWar #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA #Russians #NATO\n",
      "https://t.co/XZU7FNTK4k\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575204441770594304\n",
      "tweet = \"Video‚ñ∂Ô∏èWhy the Russian ü™ñ Army T-72 Tank is Worse Than You Think.\n",
      "#Russia #Ukraine #Putin #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #Kherson #RussianMobilization #USA #Russians #NATO\n",
      "https://t.co/XZU7FNUhTS\n",
      "\"\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575822033253867521\n",
      "tweet = #Russia #Ukraine #Europe \n",
      "Why The West Hesitant To Supply Modern Tanks Like M1 Abrams\\Leopard To Ukraine\n",
      "Ukraine #NATO Video Archive\n",
      "\n",
      "#Putin #RussianMobilization #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #USA\n",
      "https://t.co/S1Z4YOWT3k\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575642464685170688\n",
      "tweet = #Russia #Ukraine #Europe \n",
      "Why The West Hesitant To Supply Modern Tanks Like M1 Abrams\\Leopard To Ukraine\n",
      "Ukraine #NATO Video Archive.\n",
      "\n",
      "#Putin #RussianMobilization #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #USA\n",
      "https://t.co/S1Z4YOWT3k\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575482082985660422\n",
      "tweet = #Russia #Ukraine #Europe \n",
      "Why The West Hesitant To Supply Modern Tanks Like M1 Abrams\\Leopard To Ukraine\n",
      "Ukraine #NATO Video Archive\n",
      "\n",
      "#Putin #RussianMobilization #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #USA\n",
      "https://t.co/S1Z4YOFQ1k\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575458070381092866\n",
      "tweet = #Russia #Ukraine #Europe \n",
      "Why The West Hesitant To Supply Modern Tanks Like M1 Abrams\\Leopard To Ukraine\n",
      "Ukraine #NATO Video Archive\n",
      "\n",
      "#Putin #RussianMobilization #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #USA\n",
      "https://t.co/S1Z4YOFibM\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "tweet_id = 1575204591469150210\n",
      "tweet = \"Why The West Is Hesitant To Supply Modern Tanks Like M1 Abrams\\Leopard To Ukraine\n",
      "Russian-Ukraine #NATO Video Archive \n",
      "#Russia #Ukraine #Putin #RussianMobilization #Russian #Russie #Ukrainian #UkraineRussiaWar #Russland #Kharkiv #Zelensky #UkraineWar #USA\n",
      "https://t.co/S1Z4YOWT3k\n",
      "\n",
      "\n",
      "-------------------------------------------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"tank Kharkiv\"\n",
    "print(\"Query:\", query)\n",
    "results = w2v_score_index.search(query)\n",
    "top = 10\n",
    "\n",
    "print(\"\\n======================\\nSample of {} results out of {} for the searched query:\\n\".format(top, len(results)))\n",
    "for d_id in results[:top]:\n",
    "    print(\"tweet_id = {}\\ntweet = {}\".format(d_id, clean_df[clean_df[\"tweet_id\"]==d_id][\"tweet\"].item()))\n",
    "    print(\"\\n\\n-------------------------------------------------------------------------------------------\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
